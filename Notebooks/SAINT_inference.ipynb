{
 "cells": [
  {
   "source": [
    "# RIIID - SAINT+ Model Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-07T20:44:46.937500Z",
     "iopub.status.busy": "2021-01-07T20:44:46.936709Z",
     "iopub.status.idle": "2021-01-07T20:44:49.623425Z",
     "shell.execute_reply": "2021-01-07T20:44:49.622212Z"
    },
    "papermill": {
     "duration": 2.705573,
     "end_time": "2021-01-07T20:44:49.623566",
     "exception": false,
     "start_time": "2021-01-07T20:44:46.917993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "import math\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import deque, defaultdict\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:44:49.649955Z",
     "iopub.status.busy": "2021-01-07T20:44:49.649022Z",
     "iopub.status.idle": "2021-01-07T20:44:49.652307Z",
     "shell.execute_reply": "2021-01-07T20:44:49.651655Z"
    },
    "papermill": {
     "duration": 0.018043,
     "end_time": "2021-01-07T20:44:49.652402",
     "exception": false,
     "start_time": "2021-01-07T20:44:49.634359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:44:49.677784Z",
     "iopub.status.busy": "2021-01-07T20:44:49.676934Z",
     "iopub.status.idle": "2021-01-07T20:44:49.679374Z",
     "shell.execute_reply": "2021-01-07T20:44:49.679836Z"
    },
    "papermill": {
     "duration": 0.016879,
     "end_time": "2021-01-07T20:44:49.679956",
     "exception": false,
     "start_time": "2021-01-07T20:44:49.663077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AMOUNT = 100\n",
    "PAD = 0\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:44:49.707470Z",
     "iopub.status.busy": "2021-01-07T20:44:49.706797Z",
     "iopub.status.idle": "2021-01-07T20:44:49.732218Z",
     "shell.execute_reply": "2021-01-07T20:44:49.731495Z"
    },
    "papermill": {
     "duration": 0.042044,
     "end_time": "2021-01-07T20:44:49.732325",
     "exception": false,
     "start_time": "2021-01-07T20:44:49.690281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUESTIONS_PATH = '../input/riiid-test-answer-prediction/questions.csv'\n",
    "df_questions = pd.read_csv(QUESTIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:44:49.767939Z",
     "iopub.status.busy": "2021-01-07T20:44:49.767135Z",
     "iopub.status.idle": "2021-01-07T20:44:49.769999Z",
     "shell.execute_reply": "2021-01-07T20:44:49.770713Z"
    },
    "papermill": {
     "duration": 0.028129,
     "end_time": "2021-01-07T20:44:49.770834",
     "exception": false,
     "start_time": "2021-01-07T20:44:49.742705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "part_ids_map = dict(zip(df_questions.question_id, df_questions.part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010096,
     "end_time": "2021-01-07T20:44:49.791678",
     "exception": false,
     "start_time": "2021-01-07T20:44:49.781582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:44:49.818021Z",
     "iopub.status.busy": "2021-01-07T20:44:49.817346Z",
     "iopub.status.idle": "2021-01-07T20:45:29.909187Z",
     "shell.execute_reply": "2021-01-07T20:45:29.907914Z"
    },
    "papermill": {
     "duration": 40.107103,
     "end_time": "2021-01-07T20:45:29.909318",
     "exception": false,
     "start_time": "2021-01-07T20:44:49.802215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../input/saint-model/user_prepared_dataframe.pickle', 'rb') as f:\n",
    "    user_data = pickle.load(f)\n",
    "\n",
    "with open('../input/saint-model/user_index_dataframe.pickle', 'rb') as f:\n",
    "    user_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:45:29.954794Z",
     "iopub.status.busy": "2021-01-07T20:45:29.944356Z",
     "iopub.status.idle": "2021-01-07T20:45:29.971203Z",
     "shell.execute_reply": "2021-01-07T20:45:29.970688Z"
    },
    "papermill": {
     "duration": 0.050643,
     "end_time": "2021-01-07T20:45:29.971299",
     "exception": false,
     "start_time": "2021-01-07T20:45:29.920656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Riiid(torch.utils.data.Dataset):\n",
    "    \n",
    "    def generate(idx):\n",
    "        return {\n",
    "            \"user_id\": idx,\n",
    "            \"content_id\" : deque([PAD]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"answered_correctly\" : deque([PAD]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"task_container_id\" : deque([PAD]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"lagtime\" : deque([PAD]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"prior_question_elapsed_time\" : deque([PAD]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"part_id\": deque([PAD]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"padded\" : deque([True]*AMOUNT, maxlen=AMOUNT),\n",
    "            \"timestamp\": deque([PAD]*AMOUNT, maxlen=AMOUNT) # for mere calculation of lagtime\n",
    "        }\n",
    "    \n",
    "    def __init__(self, d, idxs):\n",
    "        self.d = d\n",
    "        self.idxs = idxs # we use this to locate where to locate the hit\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.d)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # you can return a dict of these as well etc etc...\n",
    "        # remember the order\n",
    "        if idx not in self.idxs:\n",
    "                self.idxs[idx] = max(self.d.keys()) + 1\n",
    "                self.d[self.idxs[idx]] = Riiid.generate(idx)\n",
    "        idx = self.idxs[idx]\n",
    "        return idx, self.d[idx][\"content_id\"], self.d[idx][\"task_container_id\"], \\\n",
    "    self.d[idx][\"part_id\"], self.d[idx][\"prior_question_elapsed_time\"], self.d[idx][\"padded\"], \\\n",
    "    self.d[idx][\"answered_correctly\"], self.d[idx]['lagtime'], self.d[idx]['timestamp']\n",
    "            \n",
    "    \n",
    "    def update(self, df):\n",
    "        #numpy array with the data, the labels are to guarantee the order\n",
    "        user_id = np.array(df['user_id'])\n",
    "        df_content_id = np.array(df['content_id'])\n",
    "        df_task_container_id = np.array(df['task_container_id'])\n",
    "        df_prior_question_elapsed_time = np.array(df['prior_question_elapsed_time'])\n",
    "        df_part_id = np.array(df['part_id'])\n",
    "        df_timestamp = np.array(df['timestamp'])\n",
    "        size = len(user_id)\n",
    "\n",
    "        for i in range(size):\n",
    "          _, content_id, task_container_id, part_id, prior_question_elapsed_time, padded, _, lagtime, timestamp = self[user_id[i]]\n",
    "          content_id.popleft()\n",
    "          task_container_id.popleft()\n",
    "          prior_question_elapsed_time.popleft()\n",
    "          part_id.popleft()\n",
    "          padded.popleft()\n",
    "          lagtime.popleft()\n",
    "          timestamp.popleft()\n",
    "          \n",
    "          content_id.append(df_content_id[i])\n",
    "          task_container_id.append(df_task_container_id[i])\n",
    "          prior_question_elapsed_time.append(df_prior_question_elapsed_time[i])\n",
    "          part_id.append(df_part_id[i])\n",
    "          padded.append(False)\n",
    "          timestamp.append(df_timestamp[i])\n",
    "          \n",
    "          # new lagtime is the difference of 2 last timestamps\n",
    "          lag = (timestamp[-1] - timestamp[-2]) // 1000\n",
    "          lagtime.append(lag if lag <= 300 else 300)\n",
    "            \n",
    "    \n",
    "    def add_answers(self, users, answers):\n",
    "        pairs = zip(users, answers)\n",
    "        for user, answer in filter(lambda x: x[1] != -1, pairs):\n",
    "            answers = self[user][-1]\n",
    "            answers.popleft()\n",
    "            answers.append(answer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    _, content_id, task_id, part_id, prior_question_elapsed_time, padded, labels, lagtime, _ = zip(*batch)\n",
    "    content_id = torch.Tensor(content_id).long()\n",
    "    task_id = torch.Tensor(task_id).long()\n",
    "    part_id = torch.Tensor(part_id).long()\n",
    "    lagtime = torch.Tensor(lagtime).long()\n",
    "    prior_question_elapsed_time = torch.Tensor(prior_question_elapsed_time).long()\n",
    "    padded = torch.Tensor(padded).bool()\n",
    "    labels = torch.Tensor(labels)\n",
    "    # remember the order\n",
    "    return content_id, task_id, part_id, prior_question_elapsed_time, padded, labels, lagtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010615,
     "end_time": "2021-01-07T20:45:29.993091",
     "exception": false,
     "start_time": "2021-01-07T20:45:29.982476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SAINT+ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:45:30.048894Z",
     "iopub.status.busy": "2021-01-07T20:45:30.033316Z",
     "iopub.status.idle": "2021-01-07T20:45:30.061064Z",
     "shell.execute_reply": "2021-01-07T20:45:30.060581Z"
    },
    "papermill": {
     "duration": 0.056833,
     "end_time": "2021-01-07T20:45:30.061185",
     "exception": false,
     "start_time": "2021-01-07T20:45:30.004352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAINT(nn.Module):\n",
    "  def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dropout, dim_feedforward, device='cpu'):\n",
    "    super(SAINT, self).__init__()\n",
    "    self.model = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout=dropout, dim_feedforward=dim_feedforward).to(device)\n",
    "    \n",
    "    # Encoder embeddings\n",
    "    self.exercise_embeddings = nn.Embedding(num_embeddings=13523, embedding_dim=d_model) # exercise_id\n",
    "    self.enc_pos_embedding = nn.Embedding(d_model, d_model) # positional embeddings\n",
    "    self.part_embeddings = nn.Embedding(num_embeddings=7+1, embedding_dim=d_model) # part_id_embeddings\n",
    "    \n",
    "    # Decoder embeddings\n",
    "    self.prior_question_elapsed_time = nn.Embedding(num_embeddings=302, embedding_dim=d_model, padding_idx=301) # prior_question_elapsed_time\n",
    "    self.dec_pos_embedding = nn.Embedding(d_model+1, d_model, padding_idx=d_model) # positional embeddings\n",
    "    self.correctness_embeddings = nn.Embedding(num_embeddings=3, embedding_dim=d_model, padding_idx=2) # Correctness embeddings\n",
    "    self.lagtime = nn.Embedding(num_embeddings=302, embedding_dim=d_model, padding_idx=301) #lag time embedding\n",
    "    \n",
    "    self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    self.device = device\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    initrange = 0.1\n",
    "    # init embeddings\n",
    "    # FIXME should be Xavier uniform acording to paper\n",
    "    self.exercise_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "    self.part_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "    self.prior_question_elapsed_time.weight.data.uniform_(-initrange, initrange)\n",
    "    self.lagtime.weight.data.uniform_(-initrange, initrange)\n",
    "    self.correctness_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "    self.enc_pos_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    self.dec_pos_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, encoder_exercises, encoder_position, encoder_part, encoder_padding, decoder_correctness, decoder_position, decoder_elapsed_time, decoder_padding, decoder_lagtime):\n",
    "    encoder_exercises = encoder_exercises.to(self.device)\n",
    "    encoder_position = encoder_position.to(self.device)\n",
    "    encoder_part = encoder_part.to(self.device)\n",
    "    decoder_correctness = decoder_correctness.to(self.device)\n",
    "    decoder_position = decoder_position.to(self.device)\n",
    "    decoder_elapsed_time = decoder_elapsed_time.to(self.device)\n",
    "    decoder_lagtime = decoder_lagtime.to(self.device)\n",
    "\n",
    "    embedding_size = encoder_exercises.shape[1] # S / T\n",
    "    mask_src = self.model.generate_square_subsequent_mask(sz=embedding_size).to(self.device)\n",
    "    mask_tgt = self.model.generate_square_subsequent_mask(sz=embedding_size).to(self.device)\n",
    "    mem_mask = self.model.generate_square_subsequent_mask(sz=embedding_size).to(self.device)\n",
    "    \n",
    "    # padded positions are masked from the self attention (when True)\n",
    "    encoder_padding = encoder_padding.bool().to(self.device)\n",
    "    decoder_padding = decoder_padding.bool().to(self.device)\n",
    "    #Memory padding mask is the same as the src\n",
    "\n",
    "    # Generate embeddings according to paper\n",
    "    embedded_src = self.exercise_embeddings(encoder_exercises) + \\\n",
    "                   self.enc_pos_embedding(encoder_position) + \\\n",
    "                   self.part_embeddings(encoder_part)\n",
    "    embedded_src = embedded_src.transpose(0, 1) # (S, N, E)\n",
    "\n",
    "    embedded_dcdr = self.correctness_embeddings(decoder_correctness) + \\\n",
    "                    self.dec_pos_embedding(decoder_position) + \\\n",
    "                    self.prior_question_elapsed_time(decoder_elapsed_time) + \\\n",
    "                    self.lagtime(decoder_lagtime)\n",
    "\n",
    "    embedded_dcdr = embedded_dcdr.transpose(0, 1) # (S, N, E)\n",
    "    \n",
    "    output = self.model(src=embedded_src, \n",
    "                        tgt=embedded_dcdr, \n",
    "                        src_mask = mask_src, \n",
    "                        tgt_mask = mask_tgt, \n",
    "                        memory_mask = mem_mask)\n",
    "                        # src_key_padding_mask = encoder_padding,\n",
    "                        # tgt_key_padding_mask = decoder_padding,\n",
    "                        # memory_key_padding_mask = encoder_padding) # TODO add padding masks\n",
    "                        # FIXME: Key padding mask not implemented. Error in Pytorch\n",
    "\n",
    "    output = self.linear(output.transpose(1, 0))\n",
    "\n",
    "    return output\n",
    "\n",
    "CORRECTNESS_DEFAULT_TOKEN = 2\n",
    "POSITION_DEFAULT_TOKEN = 100\n",
    "ELAPSED_TIME_DEFAULT_TOKEN = 301\n",
    "LAG_TIME_DEFAULT_TOKEN = 301\n",
    "\n",
    "def get_batch_embeddings(content, part, correctness, elapsed_time, lagtime, padding, device=\"cpu\"):\n",
    "  #encoder has size n, decoder has default + n-1 \n",
    "\n",
    "  size_x = content.shape[1]\n",
    "  size_y = content.shape[0]\n",
    "\n",
    "  # Encoder\n",
    "  encoder_exercises = content.long()\n",
    "  encoder_position = torch.arange(0, size_x).to(device).unsqueeze(0).repeat(size_y, 1).long()\n",
    "  encoder_part = part.long()\n",
    "  encoder_key_padding = padding.bool()\n",
    "\n",
    "  # Decoder\n",
    "  default_correct = torch.Tensor([CORRECTNESS_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n",
    "  default_position = torch.Tensor([POSITION_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n",
    "  default_elapsed_time = torch.Tensor([ELAPSED_TIME_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n",
    "  default_lagtime = torch.Tensor([LAG_TIME_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n",
    "  default_padding = torch.Tensor([True]).unsqueeze(0).repeat(size_y, 1).to(device)\n",
    "\n",
    "  decoder_correctness = torch.cat((default_correct, correctness[:,:size_x-1]), -1).long()\n",
    "  decoder_position = torch.cat((default_position, torch.arange(0, size_x-1).to(device).unsqueeze(0).repeat(size_y, 1)), -1).long()\n",
    "  decoder_elapsed_time = torch.cat((default_elapsed_time, elapsed_time[:,:size_x-1]), -1).long()\n",
    "  decoder_lagtime = torch.cat((default_lagtime, lagtime[:,:size_x-1]), -1).long()\n",
    "  decoder_key_padding = torch.cat((default_padding, padding[:,:size_x-1]), -1).bool()\n",
    "\n",
    "  return encoder_exercises, encoder_position, encoder_part, encoder_key_padding, decoder_correctness, decoder_position, decoder_elapsed_time, decoder_key_padding, decoder_lagtime"
   ]
  },
  {
   "source": [
    "## Load Pretrained Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:45:30.489206Z",
     "iopub.status.busy": "2021-01-07T20:45:30.488277Z",
     "iopub.status.idle": "2021-01-07T20:45:36.192975Z",
     "shell.execute_reply": "2021-01-07T20:45:36.193499Z"
    },
    "papermill": {
     "duration": 6.08958,
     "end_time": "2021-01-07T20:45:36.193684",
     "exception": false,
     "start_time": "2021-01-07T20:45:30.104104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = '../input/saint-model/saint_model_2.pt'\n",
    "# adam optimizer\n",
    "LEARNING_RATE = 0.001\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "WARMUP = 4000\n",
    "\n",
    "#SAINT\n",
    "N_LAYERS = 4\n",
    "WINDOW_SIZE = 100\n",
    "MODEL_DIM = 512\n",
    "DROPOUT = 0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else torch.device('cuda')\n",
    "model = SAINT(d_model=WINDOW_SIZE, nhead=5, num_encoder_layers=N_LAYERS, num_decoder_layers=N_LAYERS, dropout=DROPOUT, dim_feedforward=MODEL_DIM, device=device).to(device)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T20:45:36.243658Z",
     "iopub.status.busy": "2021-01-07T20:45:36.233734Z",
     "iopub.status.idle": "2021-01-07T20:45:37.686268Z",
     "shell.execute_reply": "2021-01-07T20:45:37.686831Z"
    },
    "papermill": {
     "duration": 1.47976,
     "end_time": "2021-01-07T20:45:37.686996",
     "exception": false,
     "start_time": "2021-01-07T20:45:36.207236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 1.4412074089050293\n"
     ]
    }
   ],
   "source": [
    "dataset = Riiid(user_data, user_idx)\n",
    "start_time = time.time()\n",
    "for index, (test_df, sample_prediction_df) in enumerate(iter_test):\n",
    "  prev_answers = eval(test_df.iloc[0]['prior_group_answers_correct'])\n",
    "  test_df = test_df[test_df.content_type_id == 0]\n",
    "  test_df[\"prior_question_elapsed_time\"].fillna(26000, inplace=True) # FIXME some random value fill in should it be like this?\n",
    "  test_df[\"prior_question_elapsed_time\"] = test_df[\"prior_question_elapsed_time\"] // 1000\n",
    "  test_df[\"prior_question_elapsed_time\"].clip(upper=300)\n",
    "  test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype(np.float16).fillna(-1).astype(np.int8)\n",
    "  test_df['part_id'] = np.array(list(map(lambda x: part_ids_map[x], test_df['content_id'])))\n",
    "\n",
    "  dataset.update(test_df)\n",
    "\n",
    "  preds = []\n",
    "  idxs = test_df['user_id']\n",
    "\n",
    "  batch = collate_fn([dataset[idx] for idx in idxs])\n",
    "  # print(batch)\n",
    "      # extract data\n",
    "  content_id, task_id, part_id, prior_question_elapsed_time, mask, labels, lagtime = batch\n",
    "  content_id = content_id.to(device)\n",
    "  task_id = task_id.to(device)\n",
    "  part_id = part_id.to(device)\n",
    "  prior_question_elapsed_time = prior_question_elapsed_time.to(device)\n",
    "  lagtime = lagtime.to(device)\n",
    "  mask = mask.to(device)\n",
    "  labels = labels.to(device)\n",
    "\n",
    "  # get embeddings\n",
    "  encoder_exercises, encoder_position, encoder_part, encoder_padding, decoder_correctness, decoder_position, decoder_elapsed_time, decoder_padding, decoder_lagtime = get_batch_embeddings(content_id, part_id, labels, prior_question_elapsed_time, lagtime, mask, device=device)\n",
    "\n",
    "  # run model \n",
    "  output = model(encoder_exercises, encoder_position, encoder_part, encoder_padding, decoder_correctness, decoder_position, decoder_elapsed_time, decoder_padding, decoder_lagtime) \n",
    "\n",
    "\n",
    "  preds = torch.sigmoid(output)[:,-1, 0].cpu().detach().numpy()\n",
    "  preds = (preds >= 0.5).astype(np.int32)\n",
    "  sample_prediction_df['answered_correctly'] =  preds\n",
    "  env.predict(sample_prediction_df[['row_id', 'answered_correctly']])\n",
    "  \n",
    "  dataset.add_answers(test_df['user_id'].values, prev_answers)\n",
    "\n",
    "print(f'Ran in {time.time() - start_time}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 56.896544,
   "end_time": "2021-01-07T20:45:39.114492",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-07T20:44:42.217948",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}