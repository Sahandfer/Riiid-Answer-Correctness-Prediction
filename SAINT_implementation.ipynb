{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SAINT_implementation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"258b75d149a44b7ba9b6d85b3a1f1e4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_66c89ee8554f42e89c14aad40811b4da","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e353bd02aee147219c56ae0c007276f6","IPY_MODEL_d48dcf6a5c7a43499de7ef31ee3d087a"]}},"66c89ee8554f42e89c14aad40811b4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e353bd02aee147219c56ae0c007276f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_99d3973f77ab46bfab8df12c35d3fe4c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_52f67be073fd4c388f16a57471a8b584"}},"d48dcf6a5c7a43499de7ef31ee3d087a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bca799553f804b7db66e23b2c8b08ee9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 393656/? [08:28&lt;00:00, 773.48it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6015cdee44054d6cbe5c2d185c81e57d"}},"99d3973f77ab46bfab8df12c35d3fe4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"52f67be073fd4c388f16a57471a8b584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bca799553f804b7db66e23b2c8b08ee9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6015cdee44054d6cbe5c2d185c81e57d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"55f19a53c6ed487395dc665d47ba9af4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5949e4acf58447739656096c4649bb64","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4648fe5d25e44def82c336635fba8743","IPY_MODEL_53cb6847b07842c6a8206bbeddadead8"]}},"5949e4acf58447739656096c4649bb64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4648fe5d25e44def82c336635fba8743":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_575656128b414988a2b2d1a575bc00b3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_99d08a5c8e6c4c39b25f877b7a7d747e"}},"53cb6847b07842c6a8206bbeddadead8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_918d5a473e33462eaab94b1a0a3f808b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3150/? [00:56&lt;00:00, 55.63it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b93342c53fe0456ebfe89b147eefc282"}},"575656128b414988a2b2d1a575bc00b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"99d08a5c8e6c4c39b25f877b7a7d747e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"918d5a473e33462eaab94b1a0a3f808b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b93342c53fe0456ebfe89b147eefc282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bbc6dcf3a9da46f58d3ea4c023d91e73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_87034081084f44759153186f9c91b702","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7e7bced364854f2e967ebae4f9f2f305","IPY_MODEL_02a296d448e64cf9a8134c86f6f382c2"]}},"87034081084f44759153186f9c91b702":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e7bced364854f2e967ebae4f9f2f305":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_03c7c5daf6ba4838b07bd540e966594e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a672f7bac5f412ba7c455f8455edacd"}},"02a296d448e64cf9a8134c86f6f382c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c20df0855614c939103d357ddf43821","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 788/? [00:06&lt;00:00, 119.69it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_27ada3e217f34294b10f13a4b9b721fc"}},"03c7c5daf6ba4838b07bd540e966594e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a672f7bac5f412ba7c455f8455edacd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c20df0855614c939103d357ddf43821":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"27ada3e217f34294b10f13a4b9b721fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"02b6a34080ad4527bd05e77b125c5bf0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_713c32dc97bd4269987009f2d6ab457f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e1b4ecee529244b0a877f2ed549a3cdc","IPY_MODEL_a1fc4c6ece3145b89d35d387ca748094"]}},"713c32dc97bd4269987009f2d6ab457f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e1b4ecee529244b0a877f2ed549a3cdc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_be46b293bb7c413fbf2d3c59ccc7a91d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_574aa2b672c04928805fcbe8116d9166"}},"a1fc4c6ece3145b89d35d387ca748094":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa5380cad207430dbf591869f36a665f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3150/? [00:57&lt;00:00, 54.81it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64d279fe5b0244b7bdb9c6c153d76c9b"}},"be46b293bb7c413fbf2d3c59ccc7a91d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"574aa2b672c04928805fcbe8116d9166":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa5380cad207430dbf591869f36a665f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"64d279fe5b0244b7bdb9c6c153d76c9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c4f0a5ae6d54621b9958661026f5466":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_309b5512f1f7415086bec4fc54ed528d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f9f2dd2130044750a941a60145f865ad","IPY_MODEL_7ac54d813e1442ab8ebf5209963e39b8"]}},"309b5512f1f7415086bec4fc54ed528d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9f2dd2130044750a941a60145f865ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_485fd006568d473886fa86d66ba6b4a3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e284e9d784b49f5b7162a0bc4e49e1b"}},"7ac54d813e1442ab8ebf5209963e39b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6ad45bae0dfc4f0b8480a3b781b86168","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 788/? [00:06&lt;00:00, 119.06it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba480ae33d114305ae515c033fadc907"}},"485fd006568d473886fa86d66ba6b4a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1e284e9d784b49f5b7162a0bc4e49e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ad45bae0dfc4f0b8480a3b781b86168":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ba480ae33d114305ae515c033fadc907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e2ec91e302846d6829045dc0901c14b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_247860d2301847cabc199cc6f8998a34","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_679c9218da4f439b86db876d48a86d84","IPY_MODEL_d9ada9c6dd894f448c53ef77f8e5bc7f"]}},"247860d2301847cabc199cc6f8998a34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"679c9218da4f439b86db876d48a86d84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_78f1cdbf617c40cdab52bc88fa0e1838","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_acbe282ad49548ac8232314180007f51"}},"d9ada9c6dd894f448c53ef77f8e5bc7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73c4a4aa2f4e4db09abc56a30861a4ec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3150/? [00:56&lt;00:00, 55.81it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85c5558591a3432aa744acd29e86d359"}},"78f1cdbf617c40cdab52bc88fa0e1838":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"acbe282ad49548ac8232314180007f51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73c4a4aa2f4e4db09abc56a30861a4ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85c5558591a3432aa744acd29e86d359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"375890e8d64b4ac6a8c1f567ec078f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6d05a5331f254ac391c920749e019082","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_605cb4968cdb499e9eb1df8b4c5d861e","IPY_MODEL_b0cb622a95ed49d587005729aecd23f9"]}},"6d05a5331f254ac391c920749e019082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"605cb4968cdb499e9eb1df8b4c5d861e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_83a831c915a24090958c58417fb6429f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a99a639a526c49e1b045235726e12bd3"}},"b0cb622a95ed49d587005729aecd23f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_35ed3fce1bcb4bad9a954d066df53fae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 788/? [00:06&lt;00:00, 119.99it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_829db71dda034b6a88d833c270fba5f0"}},"83a831c915a24090958c58417fb6429f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a99a639a526c49e1b045235726e12bd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35ed3fce1bcb4bad9a954d066df53fae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"829db71dda034b6a88d833c270fba5f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"IAFJwIdlBMOF","executionInfo":{"status":"ok","timestamp":1607943489661,"user_tz":-480,"elapsed":1090,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["import gc\n","import time\n","import math\n","import torch\n","\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn \n","import torch.nn.functional as F\n","\n","from tqdm.notebook import tqdm\n","from collections import deque, defaultdict\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"vMA1jkJ9BXyM","executionInfo":{"status":"ok","timestamp":1607943493122,"user_tz":-480,"elapsed":1302,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["TRAIN_PATH = 'Data/riiid_train.pkl.gzip'\n","QUESTIONS_PATH = 'Data/questions.csv'"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a5Knq9UBmz2","executionInfo":{"status":"ok","timestamp":1607943543267,"user_tz":-480,"elapsed":50410,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["df_train = pd.read_pickle(TRAIN_PATH)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbe3Pb7MBsHF","executionInfo":{"status":"ok","timestamp":1607943543271,"user_tz":-480,"elapsed":49878,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["df_questions = pd.read_csv(QUESTIONS_PATH)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggED0Rs9CHY1","executionInfo":{"status":"ok","timestamp":1607943549587,"user_tz":-480,"elapsed":50617,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["df_train['prior_question_had_explanation'] = df_train['prior_question_had_explanation'].astype(np.float16).fillna(-1).astype(np.int8)\n","\n","part_ids_map = dict(zip(df_questions.question_id, df_questions.part))\n","df_train['part_id'] = df_train['content_id'].map(part_ids_map)"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLhkS0kTDCYM","executionInfo":{"status":"ok","timestamp":1607943555100,"user_tz":-480,"elapsed":52093,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["df_train = df_train[df_train.content_type_id == 0]"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MImvaBWQDDgT","executionInfo":{"status":"ok","timestamp":1607943557748,"user_tz":-480,"elapsed":54232,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}},"outputId":"31665ca6-41ad-4482-fbbe-faf894aacd2a"},"source":["df_train[\"prior_question_elapsed_time\"].fillna(26000, inplace=True) # FIXME some random value fill in should it be like this?\n","df_train[\"prior_question_elapsed_time\"] = df_train[\"prior_question_elapsed_time\"] // 1000\n","df_train[\"prior_question_elapsed_time\"].clip(upper=300)"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0            26.0\n","1            37.0\n","2            55.0\n","3            19.0\n","4            11.0\n","             ... \n","101230327    18.0\n","101230328    14.0\n","101230329    14.0\n","101230330    22.0\n","101230331    29.0\n","Name: prior_question_elapsed_time, Length: 99271300, dtype: float32"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"gams0kR0FD0d","executionInfo":{"status":"ok","timestamp":1607943557749,"user_tz":-480,"elapsed":51855,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["AMOUNT = 100 # This is the parameter that gets the final AMOUNT questions of each user\n","PAD = 0 # value to use in the padding"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"20vSx-iwEZU-","executionInfo":{"status":"ok","timestamp":1607943557749,"user_tz":-480,"elapsed":50706,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["from collections import deque, defaultdict\n","def dataset_transform(dataset):\n","  final_dataset = {}\n","  user_id_to_idx = {}\n","  grp = dataset.groupby('user_id').tail(AMOUNT)\n","  \n","  for idx, row in tqdm(grp.groupby(\"user_id\").agg({\"content_id\":list, \n","                \"answered_correctly\":list, \n","                \"task_container_id\":list, \n","                \"part_id\":list, \n","                \"prior_question_elapsed_time\":list}).reset_index().iterrows()):\n","\n","    # pad the required rows to have AMOUNT values\n","    if (len(row['content_id']) >= AMOUNT):\n","      final_dataset[idx] = {\n","            \"user_id\": row[\"user_id\"],\n","            \"content_id\" : deque(row[\"content_id\"], maxlen=AMOUNT),\n","            \"answered_correctly\" : deque(row[\"answered_correctly\"], maxlen=AMOUNT),\n","            \"task_container_id\" : deque(row[\"task_container_id\"], maxlen=AMOUNT),\n","            \"prior_question_elapsed_time\" : deque(row[\"prior_question_elapsed_time\"], maxlen=AMOUNT),\n","            \"part_id\": deque(row[\"part_id\"], maxlen=AMOUNT),\n","            \"padded\" : deque([False]*100, maxlen=AMOUNT)\n","        }\n","    else: # need to pad\n","        final_dataset[idx] = {\n","            \"user_id\": row[\"user_id\"],\n","            \"content_id\" : deque([PAD]*(AMOUNT-len(row[\"content_id\"])) + row[\"content_id\"], maxlen=AMOUNT),\n","            \"answered_correctly\" : deque([PAD]*(AMOUNT-len(row[\"content_id\"])) + row[\"answered_correctly\"], maxlen=AMOUNT),\n","            \"task_container_id\" : deque([PAD]*(AMOUNT-len(row[\"content_id\"])) + row[\"task_container_id\"], maxlen=AMOUNT),\n","            \"prior_question_elapsed_time\" : deque([PAD]*(AMOUNT-len(row[\"content_id\"])) + row[\"prior_question_elapsed_time\"], maxlen=AMOUNT),\n","            \"part_id\": deque([PAD]*(AMOUNT-len(row[\"content_id\"])) + row[\"part_id\"], maxlen=AMOUNT),\n","            \"padded\" : deque([True]*(AMOUNT-len(row[\"content_id\"])) + [False]*len(row[\"content_id\"]), maxlen=AMOUNT)\n","        }\n","\n","    user_id_to_idx[row['user_id']] = idx\n","  # FIXME new users? \n","  return final_dataset, user_id_to_idx "],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["258b75d149a44b7ba9b6d85b3a1f1e4a","66c89ee8554f42e89c14aad40811b4da","e353bd02aee147219c56ae0c007276f6","d48dcf6a5c7a43499de7ef31ee3d087a","99d3973f77ab46bfab8df12c35d3fe4c","52f67be073fd4c388f16a57471a8b584","bca799553f804b7db66e23b2c8b08ee9","6015cdee44054d6cbe5c2d185c81e57d"]},"id":"3BGEaOfKSa3l","executionInfo":{"status":"ok","timestamp":1607943699139,"user_tz":-480,"elapsed":141378,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}},"outputId":"168f309a-dd97-437f-8c4f-c0da1f7231ab"},"source":["%%time \n","df_user, df_idx = dataset_transform(df_train)\n","del df_train"],"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0c7e60a756d4b58bdbef9c7d5f3cff7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nCPU times: user 1min 24s, sys: 3.34 s, total: 1min 27s\nWall time: 1min 27s\n"]}]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["hello\n"]}],"source":["dic = {1:2, 2: 3,  3:4}\n","if (2 in dic):\n","    print(\"hello\")"]},{"cell_type":"markdown","metadata":{"id":"L0RK-2CCVnq4"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"pOMaA6lHXD5A","executionInfo":{"status":"ok","timestamp":1607943709665,"user_tz":-480,"elapsed":761,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["import torch\n","import torch.nn as nn\n","\n","class SAINT(nn.Module):\n","  def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dropout, dim_feedforward, device='cpu'):\n","    super(SAINT, self).__init__()\n","    self.model = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout=dropout, dim_feedforward=dim_feedforward).to(device)\n","    \n","    # Encoder embeddings\n","    self.exercise_embeddings = nn.Embedding(num_embeddings=13523, embedding_dim=d_model) # exercise_id\n","    self.enc_pos_embedding = nn.Embedding(d_model, d_model) # positional embeddings\n","    self.part_embeddings = nn.Embedding(num_embeddings=7+1, embedding_dim=d_model) # part_id_embeddings\n","    \n","    # Decoder embeddings\n","    self.prior_question_elapsed_time = nn.Embedding(num_embeddings=301, embedding_dim=d_model) # prior_question_elapsed_time\n","    self.dec_pos_embedding = nn.Embedding(d_model, d_model) # positional embeddings\n","    self.correctness_embeddings = nn.Embedding(num_embeddings=2, embedding_dim=d_model) # Correctness embeddings\n","    # self.lag_time_embeddings = nn.Embedding(num_embeddings=?, embedding_dim=?) TODO\n","    \n","    self.linear = nn.Linear(d_model, 1)\n","\n","    self.device = device\n","    self.init_weights()\n","\n","  def init_weights(self):\n","    initrange = 0.1\n","    # init embeddings\n","    # FIXME should be Xavier uniform acording to paper\n","    self.exercise_embeddings.weight.data.uniform_(-initrange, initrange)\n","    self.part_embeddings.weight.data.uniform_(-initrange, initrange)\n","    self.prior_question_elapsed_time.weight.data.uniform_(-initrange, initrange)\n","    self.correctness_embeddings.weight.data.uniform_(-initrange, initrange)\n","    self.enc_pos_embedding.weight.data.uniform_(-initrange, initrange)\n","    self.dec_pos_embedding.weight.data.uniform_(-initrange, initrange)\n","\n","  def forward(self, encoder_exercises, encoder_position, encoder_part, decoder_correctness, decoder_position, decoder_elapsed_time, mask_src=None, mask_tgt=None):\n","    encoder_exercises = encoder_exercises.to(self.device)\n","    encoder_position = encoder_position.to(self.device)\n","    encoder_part = encoder_part.to(self.device)\n","    decoder_correctness = decoder_correctness.to(self.device)\n","    decoder_position = decoder_position.to(self.device)\n","    decoder_elapsed_time = decoder_elapsed_time.to(self.device)\n","\n","    mask_src = self.model.generate_square_subsequent_mask(sz=encoder_exercises.shape[1]).to(self.device)\n","    mask_tgt = self.model.generate_square_subsequent_mask(sz=encoder_exercises.shape[1]).to(self.device)\n","    mem_mask = self.model.generate_square_subsequent_mask(sz=encoder_exercises.shape[1]).to(self.device)\n","\n","    \n","\n","    # Generate embeddings according to paper\n","    embedded_src = self.exercise_embeddings(encoder_exercises) + \\\n","                   self.enc_pos_embedding(encoder_position) + \\\n","                   self.part_embeddings(encoder_part)\n","    embedded_src = embedded_src.transpose(0, 1) # (S, N, E)\n","\n","    embedded_dcdr = self.correctness_embeddings(decoder_correctness) + \\\n","                    self.dec_pos_embedding(decoder_position) + \\\n","                    self.prior_question_elapsed_time(decoder_elapsed_time) \n","                    # TODO add lag time embeddings\n","    embedded_dcdr = embedded_dcdr.transpose(0, 1) # (S, N, E)\n","\n","    output = self.model(src=embedded_src, tgt=embedded_dcdr, src_mask = mask_src, tgt_mask = mask_tgt, memory_mask = mem_mask)\n","    output = self.linear(output).transpose(1, 0)\n","\n","    return output\n","\n","CORRECTNESS_DEFAULT_TOKEN = 0\n","POSITION_DEFAULT_TOKEN = 1\n","ELAPSED_TIME_DEFAULT_TOKEN = 2\n","LAG_TIME_DEFAULT_TOKEN = 3\n","\n","def get_batch_embeddings(content, part, correctness, elapsed_time, device=\"cpu\"):\n","  #encoder has size n, decoder has default + n-1 \n","\n","  size_x = content.shape[1]\n","  size_y = content.shape[0]\n","\n","  # Encoder\n","  encoder_exercises = content.long()\n","  encoder_position = torch.arange(0, size_x).to(device).unsqueeze(0).repeat(size_y, 1).long()\n","  encoder_part = part.long()\n","\n","  # Decoder\n","  default_correct = torch.Tensor([CORRECTNESS_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n","  default_position = torch.Tensor([POSITION_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n","  default_elapsed_time = torch.Tensor([ELAPSED_TIME_DEFAULT_TOKEN]).unsqueeze(0).repeat(size_y, 1).to(device)\n","\n","  decoder_correctness = torch.cat((default_correct, correctness[:,:size_x-1]), -1).long()\n","  decoder_position = torch.cat((default_position, torch.arange(0, size_x-1).to(device).unsqueeze(0).repeat(size_y, 1)), -1).long()\n","  decoder_elapsed_time = torch.cat((default_elapsed_time, elapsed_time[:,:size_x-1]), -1).long()\n","  # TODO lag time\n","\n","  return encoder_exercises, encoder_position, encoder_part, decoder_correctness, decoder_position, decoder_elapsed_time"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"4B_FXut0X3-3","executionInfo":{"status":"ok","timestamp":1607943713909,"user_tz":-480,"elapsed":1056,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["class Riiid(torch.utils.data.Dataset):\n","    \n","    def __init__(self, d):\n","        self.d = d\n","    \n","    def __len__(self):\n","        return len(self.d)\n","    \n","    def __getitem__(self, idx):\n","        # you can return a dict of these as well etc etc...\n","        # remember the order\n","        return idx, self.d[idx][\"content_id\"], self.d[idx][\"task_container_id\"], \\\n","    self.d[idx][\"part_id\"], self.d[idx][\"prior_question_elapsed_time\"], self.d[idx][\"padded\"], \\\n","    self.d[idx][\"answered_correctly\"]\n","\n","def collate_fn(batch):\n","    _, content_id, task_id, part_id, prior_question_elapsed_time, padded, labels = zip(*batch)\n","    content_id = torch.Tensor(content_id).long()\n","    task_id = torch.Tensor(task_id).long()\n","    part_id = torch.Tensor(part_id).long()\n","    prior_question_elapsed_time = torch.Tensor(prior_question_elapsed_time).long()\n","    padded = torch.Tensor(padded).bool()\n","    labels = torch.Tensor(labels)\n","    # remember the order\n","    return content_id, task_id, part_id, prior_question_elapsed_time, padded, labels"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8QexncrYZps"},"source":["## Create dataset"]},{"cell_type":"code","metadata":{"id":"5hTaiGHCZsdo","executionInfo":{"status":"ok","timestamp":1607944119102,"user_tz":-480,"elapsed":2652,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["dataset = Riiid(d=df_user)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUugtvqtbGE2","executionInfo":{"status":"ok","timestamp":1607945310738,"user_tz":-480,"elapsed":1096,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["def train(model, dataloader, optimizer, criterion, device = 'cpu'):\n","  model.train()\n","\n","  train_loss = []\n","  all_labels = []\n","  preds = []\n","  num_corrects = 0.0\n","  num_total = 0.0\n","\n","  #src_mask = model.generate_square_subsequent_mask()\n","  # TODO what amount of num_workers should we use?\n","  print(f'Training in device {device}')\n","  for idx, batch in tqdm(enumerate(dataloader)):\n","\n","    # need to get the values by order as in the collate_fn\n","    content_id, task_id, part_id, prior_question_elapsed_time, mask, labels = batch\n","    content_id = content_id.to(device)\n","    task_id = task_id.to(device)\n","    part_id = part_id.to(device)\n","    prior_question_elapsed_time = prior_question_elapsed_time.to(device)\n","    mask = mask.to(device)\n","    labels = labels.to(device)\n","\n","    \n","    optimizer.zero_grad()\n","    \n","    #mask = model.generate_square_subsequent_mask(BATCH_SIZE)\n","    #print(mask.shape)\n","    output = model(content_id, part_id, labels, prior_question_elapsed_time, mask)\n","\n","    loss = criterion(output[:,:,1], labels)\n","    loss.backward()\n","    # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) Tutorial does this... but why?\n","    optimizer.step()\n","    train_loss.append(loss.detach().data.cpu().numpy())\n","\n","    output_prob = torch.sigmoid(output)[:,:,1]\n","    pred = output_prob >= 0.5\n","    # The num_corrects and num_total logic seems wrong\n","    num_corrects += (pred == labels).sum().item()\n","    num_total += np.prod(labels.shape)\n","\n","\n","    all_labels.extend(labels.cpu().numpy())\n","    preds.extend(pred.cpu().numpy())\n","\n","  acc = num_corrects / num_total\n","  auc = roc_auc_score(all_labels, preds)\n","  loss = np.mean(train_loss)\n","\n","  return loss, acc, auc\n","# train_loss, train_acc, train_auc = train(model, dataset)"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"xENpIGLWbKoV","executionInfo":{"status":"ok","timestamp":1607945184112,"user_tz":-480,"elapsed":45376,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}}},"source":["# TODO do evaluate function on dataset \n","def val(model, dataloader, criterion, device=\"cpu\"):\n","  model.eval()\n","\n","  val_loss = []\n","  all_labels = []\n","  preds = []\n","  num_corrects = 0.0\n","  num_total = 0.0\n","\n","  print(f'Evaluating in device {device}')\n","  for idx, batch in tqdm(enumerate(dataloader)):\n","    # need to get the values by order as in the collate_fn\n","    content_id, task_id, part_id, prior_question_elapsed_time, mask, labels = batch\n","    content_id = content_id.to(device)\n","    task_id = task_id.to(device)\n","    part_id = part_id.to(device)\n","    prior_question_elapsed_time = prior_question_elapsed_time.to(device)\n","    mask = mask.to(device)\n","    labels = labels.to(device)\n","\n","    #mask = model.generate_square_subsequent_mask(BATCH_SIZE)\n","    #print(mask.shape)\n","    output = model(content_id, part_id, labels, prior_question_elapsed_time, mask)\n","\n","    loss = criterion(output[:,:,1], labels)\n","    # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) Tutorial does this... but why?\n","    val_loss.append(loss.detach().data.cpu().numpy())\n","\n","    output_prob = torch.sigmoid(output)[:,:,1]\n","    pred = output_prob >= 0.5\n","    # The num_corrects and num_total logic seems wrong\n","    num_corrects += (pred == labels).sum().item()\n","    num_total += np.prod(labels.shape)\n","\n","    all_labels.extend(labels.cpu().numpy())\n","    preds.extend(pred.cpu().numpy())\n","    \n","  acc = num_corrects / num_total\n","  auc = roc_auc_score(all_labels, preds)\n","  loss = np.mean(val_loss)\n","\n","  return loss, acc, auc"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olMqFQgKzcxO","executionInfo":{"status":"ok","timestamp":1607945331039,"user_tz":-480,"elapsed":978,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}},"outputId":"9377c894-f5f0-424d-8c91-77460ca3177b"},"source":["# adam optimizer\n","LEARNING_RATE = 0.001\n","BETA_1 = 0.9\n","BETA_2 = 0.999\n","EPSILON = 1e-8\n","WARMUP = 4000\n","\n","#SAINT\n","N_LAYERS = 4\n","WINDOW_SIZE = 100\n","MODEL_DIM = 512\n","DROPOUT = 0.2\n","BATCH_SIZE = 64\n","BATCH_SIZE = AMOUNT\n","\n","criterion = nn.BCEWithLogitsLoss() #CrossEntropy is bad. but why this?\n","lr = 1e-3 # learning rate \n","\n","device = torch.device(\"cpu\" if not torch.cuda.is_available() else 'cuda')\n","\n","model = SAINT(d_model=WINDOW_SIZE, nhead=5, num_encoder_layers=N_LAYERS, num_decoder_layers=N_LAYERS, dropout=DROPOUT, dim_feedforward=MODEL_DIM, device=device).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # FIXME paper is Noam Scheme but it's not available this one is from the tutorial itself\n","\n","criterion.to(device)\n","model.to(device)"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SAINT(\n","  (model): Transformer(\n","    (encoder): TransformerEncoder(\n","      (layers): ModuleList(\n","        (0): TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","        )\n","        (1): TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","        )\n","        (2): TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","        )\n","        (3): TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): TransformerDecoder(\n","      (layers): ModuleList(\n","        (0): TransformerDecoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (multihead_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","          (dropout3): Dropout(p=0.2, inplace=False)\n","        )\n","        (1): TransformerDecoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (multihead_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","          (dropout3): Dropout(p=0.2, inplace=False)\n","        )\n","        (2): TransformerDecoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (multihead_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","          (dropout3): Dropout(p=0.2, inplace=False)\n","        )\n","        (3): TransformerDecoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (multihead_attn): MultiheadAttention(\n","            (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","          )\n","          (linear1): Linear(in_features=100, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=100, bias=True)\n","          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.2, inplace=False)\n","          (dropout2): Dropout(p=0.2, inplace=False)\n","          (dropout3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (exercise_embeddings): Embedding(13523, 100)\n","  (enc_pos_embedding): Embedding(100, 100)\n","  (part_embeddings): Embedding(8, 100)\n","  (prior_question_elapsed_time): Embedding(301, 100)\n","  (dec_pos_embedding): Embedding(100, 100)\n","  (correctness_embeddings): Embedding(2, 100)\n","  (linear): Linear(in_features=100, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":659,"referenced_widgets":["55f19a53c6ed487395dc665d47ba9af4","5949e4acf58447739656096c4649bb64","4648fe5d25e44def82c336635fba8743","53cb6847b07842c6a8206bbeddadead8","575656128b414988a2b2d1a575bc00b3","99d08a5c8e6c4c39b25f877b7a7d747e","918d5a473e33462eaab94b1a0a3f808b","b93342c53fe0456ebfe89b147eefc282","bbc6dcf3a9da46f58d3ea4c023d91e73","87034081084f44759153186f9c91b702","7e7bced364854f2e967ebae4f9f2f305","02a296d448e64cf9a8134c86f6f382c2","03c7c5daf6ba4838b07bd540e966594e","0a672f7bac5f412ba7c455f8455edacd","6c20df0855614c939103d357ddf43821","27ada3e217f34294b10f13a4b9b721fc","02b6a34080ad4527bd05e77b125c5bf0","713c32dc97bd4269987009f2d6ab457f","e1b4ecee529244b0a877f2ed549a3cdc","a1fc4c6ece3145b89d35d387ca748094","be46b293bb7c413fbf2d3c59ccc7a91d","574aa2b672c04928805fcbe8116d9166","aa5380cad207430dbf591869f36a665f","64d279fe5b0244b7bdb9c6c153d76c9b","8c4f0a5ae6d54621b9958661026f5466","309b5512f1f7415086bec4fc54ed528d","f9f2dd2130044750a941a60145f865ad","7ac54d813e1442ab8ebf5209963e39b8","485fd006568d473886fa86d66ba6b4a3","1e284e9d784b49f5b7162a0bc4e49e1b","6ad45bae0dfc4f0b8480a3b781b86168","ba480ae33d114305ae515c033fadc907","6e2ec91e302846d6829045dc0901c14b","247860d2301847cabc199cc6f8998a34","679c9218da4f439b86db876d48a86d84","d9ada9c6dd894f448c53ef77f8e5bc7f","78f1cdbf617c40cdab52bc88fa0e1838","acbe282ad49548ac8232314180007f51","73c4a4aa2f4e4db09abc56a30861a4ec","85c5558591a3432aa744acd29e86d359","375890e8d64b4ac6a8c1f567ec078f7b","6d05a5331f254ac391c920749e019082","605cb4968cdb499e9eb1df8b4c5d861e","b0cb622a95ed49d587005729aecd23f9","83a831c915a24090958c58417fb6429f","a99a639a526c49e1b045235726e12bd3","35ed3fce1bcb4bad9a954d066df53fae","829db71dda034b6a88d833c270fba5f0"]},"id":"Uq6vuHD7jViH","executionInfo":{"status":"ok","timestamp":1607945594806,"user_tz":-480,"elapsed":226543,"user":{"displayName":"Zheng Henry","photoUrl":"","userId":"01566095982561965137"}},"outputId":"1fc5cc8c-59e9-492a-e157-c235629da6b7"},"source":["# best_val_loss = float(\"inf\")\n","best_auc = 0\n","epochs = 100 # The number of epochs\n","best_model = None\n","overfitted = 0\n","\n","import time\n","\n","for epoch in range(1, epochs + 1):\n","    train_set, val_set = train_test_split(dataset, test_size=0.2)\n","\n","    train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=8)\n","    val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=8)\n","\n","    epoch_start_time = time.time()\n","    print('Training')\n","    train_loss, train_acc, train_auc = train(model=model, dataloader=train_loader, optimizer=optimizer, criterion=criterion, device=device)\n","    print(\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\".format(epoch, train_loss, train_acc, train_auc))\n","    print('Validating')\n","    val_loss, val_acc, val_auc = val(model=model, dataloader=val_loader, criterion=criterion, device=device)\n","    print(\"epoch - {} val_loss - {:.2f} acc - {:.3f} auc - {:.3f}\".format(epoch, val_loss, val_acc, val_auc))\n","    if val_auc > best_auc:\n","        best_auc = val_auc\n","        best_model = model\n","        overfitted = 0\n","    else:\n","        overfitted +=1\n","    if (overfitted >1):\n","        print(\"Model overfitted\")\n","        break\n","\n","    scheduler.step() #changes learning rate...\n"],"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Training\nTraining in device cuda\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b75f031ea6d440ea33f87ba8ba29cec"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"error","ename":"TypeError","evalue":"forward() missing 1 required positional argument: 'decoder_elapsed_time'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-217111d0f677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-258e8416fcad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#mask = model.generate_square_subsequent_mask(BATCH_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#print(mask.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_question_elapsed_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'decoder_elapsed_time'"]}]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["SAVE_PATH = \"SA(M)INT.pt\"\n","torch.save(best_model.state_dict(), SAVE_PATH)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SAINT(\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","        )\n","        (linear1): Linear(in_features=100, out_features=64, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=64, out_features=100, bias=True)\n","        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","        )\n","        (linear1): Linear(in_features=100, out_features=64, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=64, out_features=100, bias=True)\n","        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (transformer_decoder): TransformerDecoder(\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","        )\n","        (linear1): Linear(in_features=100, out_features=64, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=64, out_features=100, bias=True)\n","        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=100, out_features=100, bias=True)\n","        )\n","        (linear1): Linear(in_features=100, out_features=64, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=64, out_features=100, bias=True)\n","        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (exercise_embeddings): Embedding(13523, 100)\n","  (enc_pos_embedding): Embedding(100, 100)\n","  (part_embeddings): Embedding(8, 100)\n","  (prior_question_elapsed_time): Embedding(301, 100)\n","  (dec_pos_embedding): Embedding(100, 100)\n","  (correctness_embeddings): Embedding(2, 100)\n","  (decoder): Linear(in_features=100, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":49}],"source":["best_model"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for SAINT:\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for exercise_embeddings.weight: copying a param with shape torch.Size([13523, 100]) from checkpoint, the shape in current model is torch.Size([13523, 32]).\n\tsize mismatch for enc_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for part_embeddings.weight: copying a param with shape torch.Size([8, 100]) from checkpoint, the shape in current model is torch.Size([8, 32]).\n\tsize mismatch for prior_question_elapsed_time.weight: copying a param with shape torch.Size([301, 100]) from checkpoint, the shape in current model is torch.Size([301, 32]).\n\tsize mismatch for dec_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for correctness_embeddings.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32]).\n\tsize mismatch for decoder.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32]).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-1e1ac5dd672a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SA(M)INT.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SAINT:\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for exercise_embeddings.weight: copying a param with shape torch.Size([13523, 100]) from checkpoint, the shape in current model is torch.Size([13523, 32]).\n\tsize mismatch for enc_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for part_embeddings.weight: copying a param with shape torch.Size([8, 100]) from checkpoint, the shape in current model is torch.Size([8, 32]).\n\tsize mismatch for prior_question_elapsed_time.weight: copying a param with shape torch.Size([301, 100]) from checkpoint, the shape in current model is torch.Size([301, 32]).\n\tsize mismatch for dec_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for correctness_embeddings.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32]).\n\tsize mismatch for decoder.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32])."]}],"source":["BATCH_SIZE = 100\n","criterion = nn.BCEWithLogitsLoss() #CrossEntropy is bad. but why this?\n","lr = 1e-3 # learning rate \n","\n","model = SAINT()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # FIXME paper is Noam Scheme but it's not available this one is from the tutorial itself\n","\n","device = \"cpu\" if not torch.cuda.is_available() else torch.device('cuda:0')\n","criterion.to(device)\n","model.to(device)\n","PATH = 'SA(M)INT.pt'\n","model.load_state_dict(torch.load(PATH))"]},{"cell_type":"code","execution_count":24,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706bb07e96d74eb38112044c9d64dd46"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" ..., False,  True, False],\n","        [False, False, False,  ..., False, False,  True],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False],\n","        [ True, False,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False,  True, False],\n","        [False, False, False,  ...,  True,  True,  True],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False,  True,  ...,  True, False, False],\n","        [ True,  True, False,  ..., False,  True, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False],\n","        ...,\n","        [False, False,  True,  ..., False,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ...,  True,  True,  True]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True, False,  ..., False, False, False],\n","        [ True, False,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ..., False,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False,  True],\n","        [False,  True, False,  ..., False,  True, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False,  True,  ...,  True,  True,  True],\n","        [False,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True,  True,  True],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ...,  True,  True, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False,  True, False,  ..., False,  True,  True],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False,  True,  ...,  True, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False,  True,  ...,  True,  True, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False,  True,  ..., False,  True, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ...,  True,  True, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        [False, False, False,  ...,  True,  True, False]], device='cuda:0')\n","tensor([[ True,  True, False,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True,  True,  True],\n","        [False, False, False,  ..., False, False,  True],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False,  True,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False,  True,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False,  True],\n","        [ True,  True,  True,  ...,  True,  True, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[False,  True, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ...,  True,  True,  True],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False,  True, False,  ..., False, False, False],\n","        [False, False, False,  ...,  True, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False,  True,  True,  ..., False, False, False],\n","        [ True, False,  True,  ..., False,  True, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [False, False,  True,  ...,  True,  True, False],\n","        [False, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True, False,  ..., False, False, False],\n","        [False, False, False,  ...,  True,  True,  True],\n","        [False, False,  True,  ..., False,  True, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False,  True],\n","        [ True,  True, False,  ...,  True,  True, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False,  True,  True,  ..., False, False, False],\n","        [False, False,  True,  ..., False,  True, False],\n","        [ True,  True, False,  ...,  True,  True, False]], device='cuda:0')\n","tensor([[False,  True,  True,  ...,  True,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False, False, False,  ...,  True,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False,  True],\n","        [ True,  True,  True,  ...,  True,  True,  True],\n","        ...,\n","        [ True, False,  True,  ..., False, False,  True],\n","        [False, False,  True,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False,  True,  ..., False,  True, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        ...,\n","        [False, False,  True,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False,  True, False],\n","        [False, False,  True,  ...,  True, False, False],\n","        [False, False,  True,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ...,  True,  True,  True],\n","        [False, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False,  True, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True, False,  ...,  True, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False,  True, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ...,  True,  True, False],\n","        ...,\n","        [False, False, False,  ...,  True,  True, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False,  True,  ..., False, False,  True],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        ...,\n","        [False, False,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [False,  True, False,  ...,  True, False,  True],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False,  True],\n","        [ True, False, False,  ..., False, False,  True]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True, False,  ...,  True, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ...,  True,  True,  True],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False,  True],\n","        [ True,  True, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[False,  True,  True,  ..., False, False, False],\n","        [False, False,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [False, False,  True,  ...,  True, False,  True],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ...,  True,  True,  True],\n","        [ True, False, False,  ...,  True, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False],\n","        [ True, False,  True,  ...,  True,  True, False]], device='cuda:0')\n","tensor([[ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True,  True, False],\n","        [ True,  True,  True,  ...,  True,  True, False],\n","        ...,\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False,  True, False],\n","        [ True, False, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [ True, False, False,  ...,  True,  True,  True],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [False,  True, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ...,  True,  True, False]], device='cuda:0')\n","tensor([[False, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True,  True,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False,  True,  True],\n","        [ True,  True, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [ True,  True,  True,  ..., False, False, False],\n","        [ True,  True, False,  ..., False, False, False]], device='cuda:0')\n","tensor([[ True,  True, False,  ...,  True,  True,  True],\n","        [ True, False, False,  ..., False, False, False],\n","        [ True, False, False,  ..., False, False, False],\n","        ...,\n","        [ True, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False,  True,  True,  ..., False, False, False]], device='cuda:0')\n","\n"]}],"source":["train_set, val_set = train_test_split(dataset, test_size=0.2)\n","\n","dataloader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=8)\n","\n","for idx, batch in tqdm(enumerate(dataloader)):\n","    # need to get the values by order as in the collate_fn\n","    content_id, task_id, part_id, prior_question_elapsed_time, mask, labels = batch\n","    content_id = content_id.to(device)\n","    task_id = task_id.to(device)\n","    part_id = part_id.to(device)\n","    prior_question_elapsed_time = prior_question_elapsed_time.to(device)\n","    mask = mask.to(device)\n","    labels = labels.to(device)\n","    output = model(content_id, part_id, prior_question_elapsed_time, mask)\n","\n","    output_prob = output[:,:,1]\n","    pred = output_prob >= 0.50\n","\n","    print(pred)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for SAINT:\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for exercise_embeddings.weight: copying a param with shape torch.Size([13523, 100]) from checkpoint, the shape in current model is torch.Size([13523, 32]).\n\tsize mismatch for enc_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for part_embeddings.weight: copying a param with shape torch.Size([8, 100]) from checkpoint, the shape in current model is torch.Size([8, 32]).\n\tsize mismatch for prior_question_elapsed_time.weight: copying a param with shape torch.Size([301, 100]) from checkpoint, the shape in current model is torch.Size([301, 32]).\n\tsize mismatch for dec_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for correctness_embeddings.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32]).\n\tsize mismatch for decoder.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32]).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-e4d2c56c3cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SAINT:\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.0.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.0.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([300, 100]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([300]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 100]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.weight: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for transformer_decoder.layers.1.linear2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm2.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.weight: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_decoder.layers.1.norm3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for exercise_embeddings.weight: copying a param with shape torch.Size([13523, 100]) from checkpoint, the shape in current model is torch.Size([13523, 32]).\n\tsize mismatch for enc_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for part_embeddings.weight: copying a param with shape torch.Size([8, 100]) from checkpoint, the shape in current model is torch.Size([8, 32]).\n\tsize mismatch for prior_question_elapsed_time.weight: copying a param with shape torch.Size([301, 100]) from checkpoint, the shape in current model is torch.Size([301, 32]).\n\tsize mismatch for dec_pos_embedding.weight: copying a param with shape torch.Size([100, 100]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for correctness_embeddings.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32]).\n\tsize mismatch for decoder.weight: copying a param with shape torch.Size([2, 100]) from checkpoint, the shape in current model is torch.Size([2, 32])."]}],"source":["model1 = SAINT()\n","device = \"cpu\" if not torch.cuda.is_available() else torch.device('cuda')\n","model1 = model1.to(device)\n","model1.device = device\n","model1.load_state_dict(torch.load(SAVE_PATH, map_location=torch.device(device)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}