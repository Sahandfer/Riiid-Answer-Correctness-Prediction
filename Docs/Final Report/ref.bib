@article{Abdelrahman2019,
abstract = {Can machines trace human knowledge like humans? Knowledge tracing (KT) is a fundamental task in a wide range of applications in education, such as massive open online courses (MOOCs), intelligent tutoring systems, educational games, and learning management systems. It models dynamics in a student's knowledge states in relation to different learning concepts through their interactions with learning activities. Recently, several attempts have been made to use deep learning models for tackling the KT problem. Although these deep learning models have shown promising results, they have limitations: either lack the ability to go deeper to trace how specific concepts in a knowledge state are mastered by a student, or fail to capture long-term dependencies in an exercise sequence. In this paper, we address these limitations by proposing a novel deep learning model for knowledge tracing, namely Sequential Key-Value Memory Networks (SKVMN). This model unifies the strengths of recurrent modelling capacity and memory capacity of the existing deep learning KT models for modelling student learning. We have extensively evaluated our proposed model on five benchmark datasets. The experimental results show that (1) SKVMN outperforms the state-of-the-art KT models on all datasets, (2) SKVMN can better discover the correlation between latent concepts and questions, and (3) SKVMN can trace the knowledge state of students dynamics, and a leverage sequential dependencies in an exercise sequence for improved predication accuracy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1910.13197v1},
author = {Abdelrahman, Ghodai and Wang, Qing},
doi = {10.1145/3331184.3331195},
eprint = {arXiv:1910.13197v1},
isbn = {9781450361729},
journal = {SIGIR 2019 - Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
keywords = {Deep Learning,Key-Value Memory,Knowledge Tracing,Memory Networks,Sequence Modelling},
mendeley-groups = {ML Project},
pages = {175--184},
title = {{Knowledge tracing with sequential key-value memory networks}},
year = {2019}
}
@article{Cheng2020,
abstract = {With the rapid development of online education system, knowledge tracing which aims at predicting students' knowledge state is becoming a critical and fundamental task in personalized education. Traditionally, existing methods are domain-specified. However, there are a larger number of domains (e.g., subjects, schools) in the real world and the lacking of data in some domains, how to utilize the knowledge and information in other domains to help train a knowledge tracing model for target domains is increasingly important. We refer to this problem as domain adaptation for knowledge tracing (DAKT) which contains two aspects: (1) how to achieve great knowledge tracing performance in each domain. (2) how to transfer good performed knowledge tracing model between domains. To this end, in this paper, we propose a novel adaptable framework, namely adaptable knowledge tracing (AKT) to address the DAKT problem. Specifically, for the first aspect, we incorporate the educational characteristics (e.g., slip, guess, question texts) based on the deep knowledge tracing (DKT) to obtain a good performed knowledge tracing model. For the second aspect, we propose and adopt three domain adaptation processes. First, we pre-train an auto-encoder to select useful source instances for target model training. Second, we minimize the domain-specific knowledge state distribution discrepancy under maximum mean discrepancy (MMD) measurement to achieve domain adaptation. Third, we adopt fine-tuning to deal with the problem that the output dimension of source and target domain are different to make the model suitable for target domains. Extensive experimental results on two private datasets and seven public datasets clearly prove the effectiveness of AKT for great knowledge tracing performance and its superior transferable ability.},
archivePrefix = {arXiv},
arxivId = {2001.04841},
author = {Cheng, Song and Liu, Qi and Chen, Enhong},
eprint = {2001.04841},
mendeley-groups = {ML Project},
number = {July 2017},
title = {{Domain Adaption for Knowledge Tracing}},
url = {http://arxiv.org/abs/2001.04841},
year = {2020}
}
@article{Choi2020,
archivePrefix = {arXiv},
arxivId = {2002.07033},
author = {Choi, Youngduck and Lee, Youngnam and Cho, Junghyun and Baek, Jineon and Kim, Byungsoo and Cha, Yeongmin and Shin, Dongmin and Bae, Chan and Heo, Jaewe},
doi = {10.1145/3386527.3405945},
eprint = {2002.07033},
isbn = {9781450379519},
mendeley-groups = {ML Project},
pages = {341--344},
title = {{Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing}},
year = {2020}
}


@misc{Corbett1995,
abstract = {This paper describes an effort to model students' changing knowledge state during skill acquisition.  Students in this research are learning to write short programs with the ACT Programming Tutor (APT).  APT is constructed around a production rule cognitive model of programming knowledge, called the *ideal student model*.  This model allows the tutor to solve exercises along with the student and provide assistance as necessary.  As the student works, the tutor also maintains an estimate of the probability that the student has learned each of the rules in the ideal model, in a process called *knowledge tracing*.  The tutor presents an individualized sequence of exercises to the student based on these probability estimates until the student has 'mastered' each rule.  The programming tutor, cognitive model and learning and performance assumptions are described.  A series of studies is reviewed that examine the empirical validity of knowledge tracing and has led to modificiations in the process.  Currently the model is quite successful in predicting test performance.  Further modifications in the modeling process are discussed that may improve performance levels.},
author = {Corbett, A T and Anderson, John R},
booktitle = {User modeling and user-adapted interaction},
keywords = {empirical validity,individual differences,intelligent tutoring systems,learning,mastery learning,procedural knolwedge,student modeling},
mendeley-groups = {ML Project},
number = {4},
pages = {253--278},
title = {{Knowledge Tracing }},
url = {https://link.springer.com/article/10.1007/BF01099821},
volume = {4},
year = {1995}
}
@article{Ding2020,
abstract = {Most existing research works involving deep learning focus on performance improvement by developing new architectures or regularizers. However, in this paper we study the modeling of uncertainty in recurrent networks for the application of student response modeling, more specifically, knowledge tracing. Knowledge tracing is an application of time series machine learning. It consists of inferring the mastery level of a skill for a student as they navigate a question bank, thus adjusting curriculum for efficient learning. Deep Knowledge Tracing (DKT) takes the deep learning approach for knowledge tracing and has achieved better results than models like Bayesian Knowledge Tracing (BKT) and Performance Factor Analysis (PFA). However, the opaqueness of these deep knowledge tracing models also brings some criticisms. Providing an uncertainty score for each prediction helps mitigate this opaqueness. To investigate uncertainty modeling in DKT, we first examine a popular way of modeling data dependent uncertainties using Monte Carlo and show how it is insufficient to model variance in data. Second, we show how to incorporate sensible uncertainties by explicitly regularizing the cross entropy loss function. Third, we evaluate our method both in three different real datasets and in a more controlled way using synthetic data. Using synthetic data allows us to quantitatively understand the generated uncertainties. The results show that our method provides comparable results with standard deep knowledge tracing models as well as meaningful prediction uncertainties.},
author = {Ding, Xinyi and Larson, Eric C.},
doi = {10.1016/j.neucom.2020.05.035},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep learning,Knowledge tracing,Recurrent neural network,Student response modeling,Uncertainty},
mendeley-groups = {ML Project},
pages = {74--82},
publisher = {Elsevier B.V.},
title = {{Incorporating uncertainties in student response modeling by loss function regularization}},
url = {https://doi.org/10.1016/j.neucom.2020.05.035},
volume = {409},
year = {2020}
}
@article{Khajah2016,
abstract = {In theoretical cognitive science, there is a tension between highly structured models whose parameters have a direct psychological interpretation and highly complex, general-purpose models whose parameters and representations are difficult to interpret. The former typically provide more insight into cognition but the latter often perform better. This tension has recently surfaced in the realm of educational data mining, where a deep learning approach to predicting students' performance as they work through a series of exercises—termed deep knowledge tracing or DKT—has demonstrated a stunning performance advantage over the mainstay of the field, Bayesian knowledge tracing or BKT. In this article, we attempt to understand the basis for DKT's advantage by considering the sources of statistical regularity in the data that DKT can leverage but which BKT cannot. We hypothesize four forms of regularity that BKT fails to exploit: recency effects, the contextualized trial sequence, inter-skill similarity, and individual variation in ability. We demonstrate that when BKT is extended to allow it more flexibility in modeling statistical regularities—using extensions previously proposed in the literature—BKT achieves a level of performance indistinguishable from that of DKT. We argue that while DKT is a powerful, useful, general-purpose framework for modeling student learning, its gains do not come from the discovery of novel representations—the fundamental advantage of deep learning. To answer the question posed in our title, knowledge tracing may be a domain that does not require ‘depth'; shallow models like BKT can perform just as well and offer us greater interpretability and explanatory power.},
archivePrefix = {arXiv},
arxivId = {1604.02416},
author = {Khajah, Mohammad and Lindsey, Robert V. and Mozer, Michael C.},
eprint = {1604.02416},
journal = {Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016},
mendeley-groups = {ML Project},
pages = {94--101},
title = {{How deep is knowledge tracing?}},
year = {2016}
}
@article{Liu2018,
abstract = {Knowledge tracing (KT)[1] has been used in various forms for adaptive computerized instruction for more than 40 years. However, despite its long history of application, it is difficult to use in domain model search procedures, has not been used to capture learning where multiple skills are needed to perform a single action, and has not been used to compute latencies of actions. On the other hand, existing models used for educational data mining (e.g. Learning Factors Analysis (LFA)[2]) and model search do not tend to allow the creation of a “model overlay” that traces predictions for individual students with individual skills so as to allow the adaptive instruction to automatically remediate performance. Because these limitations make the transition from model search to model application in adaptive instruction more difficult, this paper describes our work to modify an existing data mining model so that it can also be used to select practice adaptively. We compare this new adaptive data mining model (PFA, Performance Factors Analysis) with two versions of LFA and then compare PFA with standard KT},
author = {Liu, Lizhong and Jia, Jinping and Sun, Tonghua and Zhang, Hongbo},
isbn = {09226389 (ISSN); 9781607500285 (ISBN)},
issn = {18734979},
journal = {Materials Letters},
keywords = {adaptive modeling,educational data mining,knowledge tracing},
mendeley-groups = {ML Project},
pages = {107--110},
title = {{Performance Factors Analysis – A New Alternative to Knowledge Tracing}},
volume = {212},
year = {2018}
}
@article{Liu2019,
abstract = {For offering proactive services to students in intelligent education, one of the fundamental tasks is predicting their performance (e.g., scores) on future exercises, where it is necessary to track each student's knowledge acquisition during her exercising activities. However, existing approaches can only exploit the exercising records of students, and the problem of extracting rich information existed in the exercise's materials (e.g., knowledge concepts, exercise content) to achieve both precise predictions of student performance and interpretable analysis of knowledge acquisition remains underexplored. In this paper, we present a holistic study of student performance prediction. To directly achieve the primary goal of prediction, we first propose a general Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both student's records and the exercise contents. In EERNN, we simply summarize each student's state into an integrated vector and trace it with a recurrent neural network, where we design a bidirectional LSTM to learn the encoding of each exercise's content. For making predictions, we propose two implementations under EERNN with different strategies, i.e., EERNNM with Markov property and EERNNA with Attention mechanism. Then, to explicitly track student's knowledge acquisition on multiple knowledge concepts, we extend EERNN to an explainable Exercise-aware Knowledge Tracing (EKT) by incorporating the knowledge concept effects, where the student's integrated state vector is extended to a knowledge state matrix. In EKT, we further develop a memory network for quantifying how much each exercise can affect the mastery of students on concepts during the exercising process. Finally, we conduct extensive experiments on large-scale real-world data. The results demonstrate the prediction effectiveness of two frameworks as well as the superior interpretability of EKT.},
archivePrefix = {arXiv},
arxivId = {1906.05658},
author = {qi Liu and Huang, Zhenya and Yin, Yu and Chen, Enhong and Xiong, Hui and Su, Yu and Hu, Guoping},
doi = {10.1109/tkde.2019.2924374},
eprint = {1906.05658},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
mendeley-groups = {ML Project},
pages = {1--1},
title = {{EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction}},
year = {2019}
}
@article{Minn2018,
abstract = {In Intelligent Tutoring System (ITS), tracing the student's knowledge state during learning has been studied for several decades in order to provide more supportive learning instructions. In this paper, we propose a novel model for knowledge tracing that i) captures students' learning ability and dynamically assigns students into distinct groups with similar ability at regular time intervals, and ii) combines this information with a Recurrent Neural Network architecture known as Deep Knowledge Tracing. Experimental results confirm that the proposed model is significantly better at predicting student performance than well known state-of-the-art techniques for student modelling.},
archivePrefix = {arXiv},
arxivId = {1809.08713},
author = {Minn, Sein and Yu, Yi and Desmarais, Michel C. and Zhu, Feida and Vie, Jill Jenn},
doi = {10.1109/ICDM.2018.00156},
eprint = {1809.08713},
isbn = {9781538691588},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Deep knowledge tracing,K-means clustering,LSTMs,RNNs,Student model},
mendeley-groups = {ML Project},
pages = {1182--1187},
title = {{Deep Knowledge Tracing and Dynamic Student Classification for Knowledge Tracing}},
volume = {2018-Novem},
year = {2018}
}
@article{Nakagawa2019,
abstract = {Recent advancements in computer-assisted learning systems have caused an increase in the research of knowledge tracing, wherein student performance on coursework exercises is predicted over time. From the viewpoint of data structure, the coursework can be potentially structured as a graph. Incorporating this graph-structured nature into the knowledge tracing model as a relational inductive bias can improve its performance; however, previous methods, such as deep knowledge tracing, did not consider such a latent graph structure. Inspired by the recent successes of the graph neural network (GNN), we herein propose a GNN-based knowledge tracing method, i.e., graph-based knowledge tracing. Casting the knowledge structure as a graph enabled us to reformulate the knowledge tracing task as a time-series node-level classification problem in the GNN. As the knowledge graph structure is not explicitly provided in most cases, we propose various implementations of the graph structure. Empirical validations on two open datasets indicated that our method could potentially improve the prediction of student performance and demonstrated more interpretable predictions compared to those of the previous methods, without the requirement of any additional information.},
author = {Nakagawa, Hiromi and Iwasawa, Yusuke and Matsuo, Yutaka},
doi = {10.1145/3350546.3352513},
isbn = {9781450369343},
journal = {Proceedings - 2019 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2019},
keywords = {Educational data mining,Graph neural network,Knowledge tracing,Learning sciences},
mendeley-groups = {ML Project},
number = {2016},
pages = {156--163},
title = {{Graph-based knowledge tracing: Modeling student proficiency using graph neural network}},
year = {2019}
}
@article{Pandey2019,
abstract = {Knowledge tracing is the task of modeling each student's mastery of knowledge concepts (KCs) as (s)he engages with a sequence of learning activities. Each student's knowledge is modeled by estimating the performance of the student on the learning activities. It is an important research area for providing a personalized learning platform to students. In recent years, methods based on Recurrent Neural Networks (RNN) such as Deep Knowledge Tracing (DKT) and Dynamic Key-Value Memory Network (DKVMN) outperformed all the traditional methods because of their ability to capture a complex representation of human learning. However, these methods face the issue of not generalizing well while dealing with sparse data which is the case with real-world data as students interact with few KCs. In order to address this issue, we develop an approach that identifies the KCs from the student's past activities that are relevant to the given KC and predicts his/her mastery based on the relatively few KCs that it picked. Since predictions are made based on relatively few past activities, it handles the data sparsity problem better than the methods based on RNN. For identifying the relevance between the KCs, we propose a self-attention based approach, Self Attentive Knowledge Tracing (SAKT). Extensive experimentation on a variety of real-world dataset shows that our model outperforms the state-of-the-art models for knowledge tracing, improving AUC by 4.43{\%} on average.},
archivePrefix = {arXiv},
arxivId = {1907.06837},
author = {Pandey, Shalini and Karypis, George},
eprint = {1907.06837},
isbn = {9781733673600},
journal = {EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining},
keywords = {Knowledge tracing,Massive open online courses,Self-attention,Sequential recommendation},
mendeley-groups = {ML Project},
pages = {384--389},
title = {{A self-attentive model for knowledge tracing}},
year = {2019}
}
@article{Pandey2020,
abstract = {The world has transitioned into a new phase of online learning in response to the recent Covid19 pandemic. Now more than ever, it has become paramount to push the limits of online learning in every manner to keep flourishing the education system. One crucial component of online learning is Knowledge Tracing (KT). The aim of KT is to model student's knowledge level based on their answers to a sequence of exercises referred as interactions. Students acquire their skills while solving exercises and each such interaction has a distinct impact on student ability to solve a future exercise. This $\backslash$textit{\{}impact{\}} is characterized by 1) the relation between exercises involved in the interactions and 2) student forget behavior. Traditional studies on knowledge tracing do not explicitly model both the components jointly to estimate the impact of these interactions. In this paper, we propose a novel Relation-aware self-attention model for Knowledge Tracing (RKT). We introduce a relation-aware self-attention layer that incorporates the contextual information. This contextual information integrates both the exercise relation information through their textual content as well as student performance data and the forget behavior information through modeling an exponentially decaying kernel function. Extensive experiments on three real-world datasets, among which two new collections are released to the public, show that our model outperforms state-of-the-art knowledge tracing methods. Furthermore, the interpretable attention weights help visualize the relation between interactions and temporal patterns in the human learning process.},
archivePrefix = {arXiv},
arxivId = {2008.12736},
author = {Pandey, Shalini and Srivastava, Jaideep},
doi = {10.1145/3340531.3411994},
eprint = {2008.12736},
isbn = {9781450368599},
mendeley-groups = {ML Project},
title = {{RKT : Relation-Aware Self-Attention for Knowledge Tracing}},
url = {http://arxiv.org/abs/2008.12736{\%}0Ahttp://dx.doi.org/10.1145/3340531.3411994},
year = {2020}
}
@article{Pardos2010,
abstract = {The field of intelligent tutoring systems has been using the well known knowledge tracing model, popularized by Corbett and Anderson (1995), to track student knowledge for over a decade. Surprisingly, models currently in use do not allow for individual learning rates nor individualized estimates of student initial knowledge. Corbett and Anderson, in their original articles, were interested in trying to add individualization to their model which they accomplished but with mixed results. Since their original work, the field has not made significant progress towards individualization of knowledge tracing models in fitting data. In this work, we introduce an elegant way of formulating the individualization problem entirely within a Bayesian networks framework that fits individualized as well as skill specific parameters simultaneously, in a single step. With this new individualization technique we are able to show a reliable improvement in prediction of real world data by individualizing the initial knowledge parameter. We explore three difference strategies for setting the initial individualized knowledge parameters and report that the best strategy is one in which information from multiple skills is used to inform each student's prior. Using this strategy we achieved lower prediction error in 33 of the 42 problem sets evaluated. The implication of this work is the ability to enhance existing intelligent tutoring systems to more accurately estimate when a student has reached mastery of a skill. Adaptation of instruction based on individualized knowledge and learning speed is discussed as well as open research questions facing those that wish to exploit student and skill information in their user models. {\textcopyright} 2010 Springer-Verlag.},
author = {Pardos, Zachary A. and Heffernan, Neil T.},
doi = {10.1007/978-3-642-13470-8_24},
isbn = {3642134696},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bayesian Networks,Data Mining,Individualization,Intelligent Tutoring Systems,Knowledge Tracing,Prediction},
mendeley-groups = {ML Project},
pages = {255--266},
title = {{Modeling individualization in a Bayesian networks implementation of knowledge tracing}},
volume = {6075 LNCS},
year = {2010}
}
@article{Pavlik2019,
author = {Pavlik, Philip I and Eglington, Luke G and Harrell-williams, Leigh M},
mendeley-groups = {ML Project},
title = {{Generalized Knowledge Tracing : A Constrained Framework for Learner Modeling}},
year = {2019}
}
@article{Pu2020,
abstract = {In this work, we propose a Transformer-based model to trace students' knowledge acquisition. We modified the Transformer structure to utilize 1) the association between questions and skills and 2) the elapsed time between question steps. The use of question-skill associations allows the model to learn specific representation for frequently encountered questions while representing rare questions with their underline skill representations. The inclusion of elapsed time opens the opportunity to address forgetting. Our approach outperforms the state-of-the-art methods in the literature by roughly 10{\%} in AUC with frequently used public datasets.},
author = {Pu, Shi and Yudelson, Michael and Ou, Lu and Huang, Yuchi},
doi = {10.1007/978-3-030-52240-7_46},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bayesian Knowledge Tracing,Deep Knowledge Tracing,Transformer},
mendeley-groups = {ML Project},
number = {July},
pages = {252--256},
title = {{Deep Knowledge Tracing with Transformers}},
volume = {12164 LNAI},
year = {2020}
}
@article{Scruggs2019,
archivePrefix = {arXiv},
arxivId = {1910.12597},
author = {Scruggs, Richard and Baker, Ryan S. and McLaren, Bruce M.},
eprint = {1910.12597},
keywords = {Bayesian knowledge tracing,Deep knowledge tracing,dynamic key-value memory networks,latent knowledge,performance factors analysis},
mendeley-groups = {ML Project},
title = {{Extending Deep Knowledge Tracing: Inferring Interpretable Knowledge and Predicting Post-System Performance}},
url = {http://arxiv.org/abs/1910.12597},
year = {2019}
}
@article{Tong2020,
abstract = {Knowledge tracing (KT) which aims at predicting learner's knowledge mastery plays an important role in the computer-aided educational system. Given learners' exercise records, a knowledge tracing model can trace their hidden knowledge state dynamically. In recent years, many deep learning models have been applied to tackle the KT task, which has shown promising results. However, they still have limitations. Most existing methods simplify the exercising records as knowledge sequence, which fails to explore rich information existed in exercise texts. Besides, the latent hierarchical graph nature of exercises and knowledge remain unexplored. Thus, in this paper, we propose a hierarchical graph knowledge tracing model framework (HGKT) which could leverage the advantages of hierarchical exercise graph and sequence model to enhance the ability of knowledge tracing. Besides, we introduce the concept of problem schema to better represent a group of similar exercises and propose a hierarchical graph neural network to learn representations of problem schemas. Moreover, in the sequence model, we employ two attention mechanisms to highlight important historical states of students. In the testing stage, we present a K{\&}S diagnosis matrix that could trace the transition of mastery of knowledge and problem schema, which could more easily be applied to different applications. Finally, we conduct extensive experiments to evaluate the model on a large scale real-world dataset. The results prove the effectiveness of our model and the diversity of its application scenarios.},
archivePrefix = {arXiv},
arxivId = {2006.16915},
author = {Tong, Hanshuang and Zhou, Yun and Wang, Zhen},
eprint = {2006.16915},
keywords = {a ention,archical exercise graph,graph convolutional network,hier-,intelligent education,knowledge tracing,problem schema},
mendeley-groups = {ML Project},
title = {{HGKT : Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge Tracing}},
url = {http://arxiv.org/abs/2006.16915},
year = {2020}
}
@article{Wang2019,
abstract = {Monitoring student knowledge states or skill acquisition levels known as knowledge tracing, is a fundamental part of intelligent tutoring systems. Despite its inherent challenges, recent deep neural networks based knowledge tracing models have achieved great success, which is largely from models' ability to learn sequential dependencies of questions in student exercise data. However, in addition to sequential information, questions inherently exhibit side relations, which can enrich our understandings about student knowledge states and has great potentials to advance knowledge tracing. Thus, in this paper, we exploit side relations to improve knowledge tracing and design a novel framework DTKS. The experimental results on real education data validate the effectiveness of the proposed framework and demonstrate the importance of side information in knowledge tracing.},
archivePrefix = {arXiv},
arxivId = {1909.00372},
author = {Wang, Zhiwei and Feng, Xiaoqin and Tang, Jiliang and Huang, Gale Yan and Liu, Zitao},
doi = {10.1007/978-3-030-23207-8_56},
eprint = {1909.00372},
isbn = {9783030232061},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {ML Project},
pages = {303--308},
title = {{Deep knowledge tracing with side information}},
volume = {11626 LNAI},
year = {2019}
}
@article{Yang2020,
abstract = {Knowledge tracing (KT) has recently been an active research area of computational pedagogy. The task is to model students mastery level of knowledge based on their responses to the questions in the past, as well as predict the probabilities that they correctly answer subsequent questions in the future. A good KT model can not only make students timely aware of their knowledge states, but also help teachers develop better personalized teaching plans for students. KT tasks were historically solved using statistical modeling methods such as Bayesian inference and factor analysis, but recent advances in deep learning have led to the successive proposals that leverage deep neural networks, including long short-term memory networks, memory-augmented networks and self-attention networks. While those deep models demonstrate superior performance over the traditional approaches, they all neglect more or less the impact on knowledge states of the most recent questions answered by students. The forgetting curve theory states that human memory retention declines over time, therefore knowledge states should be mostly affected by the recent questions. Based on this observation, we propose a Convolutional Knowledge Tracing (CKT) model in this paper. In addition to modeling the long-term effect of the entire question-answer sequence, CKT also strengthens the short-term effect of recent questions using 3D convolutions, thereby effectively modeling the forgetting curve in the learning process. Extensive experiments show that CKT achieves the new state-of-the-art in predicting students performance compared with existing models. Using CKT, we gain 1.55 and 2.03 improvements in terms of AUC over DKT and DKVMN respectively, on the ASSISTments2009 dataset. And on the ASSISTments2015 dataset, the corresponding improvements are 1.01 and 1.96 respectively.},
archivePrefix = {arXiv},
arxivId = {2008.01169},
author = {Yang, Shanghui and Zhu, Mengxia and Hou, Jingyang and Lu, Xuesong},
eprint = {2008.01169},
keywords = {acm reference format,convolution neural network,deep knowledge tracing,education data mining,modelling,sequence},
mendeley-groups = {ML Project},
title = {{Deep Knowledge Tracing with Convolutions}},
url = {http://arxiv.org/abs/2008.01169},
year = {2020}
}
@article{Yang2009,
abstract = {With the rapid development in online education, knowledge tracing (KT) has become a fundamental problem which traces students' knowledge status and predicts their performance on new questions. Questions are often numerous in online education systems, and are always associated with much fewer skills. However, the previous literature fails to involve question information together with high-order question-skill correlations, which is mostly limited by data sparsity and multi-skill problems. From the model perspective, previous models can hardly capture the long-term dependency of student exercise history, and cannot model the interactions between student-questions, and student-skills in a consistent way. In this paper, we propose a Graph-based Interaction model for Knowledge Tracing (GIKT) to tackle the above probems. More specifically, GIKT utilizes graph convolutional network (GCN) to substantially incorporate question-skill correlations via embedding propagation. Besides, considering that relevant questions are usually scattered throughout the exercise history, and that question and skill are just different instantiations of knowledge, GIKT generalizes the degree of students' master of the question to the interactions between the student's current state, the student's history related exercises, the target question, and related skills. Experiments on three datasets demonstrate that GIKT achieves the new state-of-the-art performance, with at least 1{\%} absolute AUC improvement.},
archivePrefix = {arXiv},
arxivId = {2009.05991},
author = {Yang, Yang and Shen, Jian and Qu, Yanru and Liu, Yunfei and Wang, Kerong and Zhu, Yaoming and Zhang, Weinan and Yu, Yong},
eprint = {2009.05991},
keywords = {graph neural network,information,knowledge tracing},
mendeley-groups = {ML Project},
pages = {1--17},
title = {{GIKT: A Graph-based Interaction Model for Knowledge Tracing}},
url = {http://arxiv.org/abs/2009.05991},
volume = {1},
year = {2020}
}
@article{Xiong2016,
abstract = {Over the last couple of decades, there have been a large variety of approaches towards modeling student knowledge within intelligent tutoring systems. With the booming development of deep learning and large-scale artificial neural networks, there have been empirical successes in a number of machine learning and data mining applications, including student knowledge modeling. Deep Knowledge Tracing (DKT), a pioneer algorithm that utilizes recurrent neural networks to model student learning, reports substantial improvements in prediction performance. To help the EDM community better understand the promising techniques of deep learning, we examine DKT alongside two well-studied models for knowledge modeling, PFA and BKT. In addition to sharing a primer on the internal computational structures of DKT, we also report on potential issues that arise from data formatting. We take steps to reproduce the experiments of Deep Knowledge Tracing by implementing a DKT algorithm using Google's TensorFlow framework; we also reproduce similar results on new datasets. We determine that the DKT findings don't hold an overall edge when compared to the PFA model, when applied to properly prepared datasets that are limited to main (i.e. non-scaffolding) questions. More importantly, during the investigation of DKT, we not only discovered a data quality issue in a public available data set, but we also detected a vulnerability of DKT at how it handles multiple skill sequences.},
author = {Xiong, Xiaolu and Zhao, Siyuan and {Van Inwegen}, Eric G. and Beck, Joseph E.},
journal = {Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016},
keywords = {Data quality,Deep learning,Knowledge tracing,Performance factors analysis,Recurrent neural networks,Student modeling},
pages = {545--550},
title = {{Going deeper with deep knowledge tracing}},
year = {2016}
}
@article{Baker2008,
author = {Baker, Ryan S.J.D. and Corbett, Albert T. and Aleven, Vincent},
doi = {10.1007/978-3-540-69132-7-44},
journal = {Lecture Notes in Computer Science},
pages = {406--415},
title = {{More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing}},
volume = {5091 LNCS},
year = {2008}
}
@article{Tong2006,
abstract = {Knowledge tracing (KT) which aims at predicting learner's knowledge mastery plays an important role in the computer-aided educational system. Given learners' exercise records, a knowledge tracing model can trace their hidden knowledge state dynamically. In recent years, many deep learning models have been applied to tackle the KT task, which has shown promising results. However, they still have limitations. Most existing methods simplify the exercising records as knowledge sequence, which fails to explore rich information existed in exercise texts. Besides, the latent hierarchical graph nature of exercises and knowledge remain unexplored. Thus, in this paper, we propose a hierarchical graph knowledge tracing model framework (HGKT) which could leverage the advantages of hierarchical exercise graph and sequence model to enhance the ability of knowledge tracing. Besides, we introduce the concept of problem schema to better represent a group of similar exercises and propose a hierarchical graph neural network to learn representations of problem schemas. Moreover, in the sequence model, we employ two attention mechanisms to highlight important historical states of students. In the testing stage, we present a K{\&}S diagnosis matrix that could trace the transition of mastery of knowledge and problem schema, which could more easily be applied to different applications. Finally, we conduct extensive experiments to evaluate the model on a large scale real-world dataset. The results prove the effectiveness of our model and the diversity of its application scenarios.},
archivePrefix = {arXiv},
arxivId = {2006.16915},
author = {Tong, Hanshuang and Zhou, Yun and Wang, Zhen},
eprint = {2006.16915},
keywords = {a ention,archical exercise graph,graph convolutional network,hier-,intelligent education,knowledge tracing,problem schema},
title = {{HGKT : Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge Tracing}},
url = {http://arxiv.org/abs/2006.16915},
year = {2020}
}
@article{Piech2015,
abstract = {Knowledge tracing-where a machine models the knowledge of a student as they interact with coursework-is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.},
archivePrefix = {arXiv},
arxivId = {1506.05908},
author = {Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and Sohl-Dickstein, Jascha},
eprint = {1506.05908},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {505--513},
title = {{Deep knowledge tracing}},
volume = {2015-Janua},
year = {2015}
}
@article{Zhang2017,
abstract = {Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.08108v2},
author = {Zhang, Jiani and Shi, Xingjian and King, Irwin and Yeung, Dit Yan},
doi = {10.1145/3038912.3052580},
eprint = {arXiv:1611.08108v2},
isbn = {9781450349130},
journal = {26th International World Wide Web Conference, WWW 2017},
keywords = {Concept discovery,Deep learning,Dynamic key-value memory networks,Knowledge tracing,Massive open online courses},
pages = {765--774},
title = {{Dynamic key-value memory networks for knowledge tracing}},
year = {2017}
}

@article{Shin2020,
abstract = {We propose SAINT+, a successor of SAINT which is a Transformer based knowledge tracing model that separately processes exercise information and student response information. Following the architecture of SAINT, SAINT+ has an encoder-decoder structure where the encoder applies self-attention layers to a stream of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to streams of response embeddings and encoder output. Moreover, SAINT+ incorporates two temporal feature embeddings into the response embeddings: elapsed time, the time taken for a student to answer, and lag time, the time interval between adjacent learning activities. We empirically evaluate the effectiveness of SAINT+ on EdNet, the largest publicly available benchmark dataset in the education domain. Experimental results show that SAINT+ achieves state-of-the-art performance in knowledge tracing with an improvement of 1.25{\%} in area under receiver operating characteristic curve compared to SAINT, the current state-of-the-art model in EdNet dataset.},
archivePrefix = {arXiv},
arxivId = {2010.12042},
author = {Shin, Dongmin and Shim, Yugeun and Yu, Hangyeol and Lee, Seewoo and Kim, Byungsoo and Choi, Youngduck},
eprint = {2010.12042},
file = {:Users/sahandsabour/Desktop/ML project/Papers/SAINT+- Integrating Temporal Features for EdNet Correctness Prediction.pdf:pdf},
pages = {1--10},
title = {{SAINT+: Integrating Temporal Features for EdNet Correctness Prediction}},
url = {http://arxiv.org/abs/2010.12042},
year = {2020}
}

@inproceedings{Vaswani2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {5998--6008},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}



