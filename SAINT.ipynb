{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque, defaultdict\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "## 1. Import datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "TRAIN_PATH = 'Data/riiid_train.pkl.gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT = 100 # Number of questions per user\n",
    "PAD = 0 # Value for padding\n",
    "BATCH_SIZE = 100\n",
    "NUM_ENCODER = 4\n",
    "NUM_DECODER = 4\n",
    "MAX_SEQ = 100\n",
    "\n",
    "EMBED_DIMS = 32\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "TOTAL_EXE = 13523 # number of unique questions\n",
    "TOTAL_CAT = 10000\n",
    "\n",
    "DEVICE = 'cpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 101230332 entries, 0 to 101230331\nData columns (total 10 columns):\n #   Column                          Dtype  \n---  ------                          -----  \n 0   row_id                          int64  \n 1   timestamp                       int64  \n 2   user_id                         int32  \n 3   content_id                      int16  \n 4   content_type_id                 bool   \n 5   task_container_id               int16  \n 6   user_answer                     int8   \n 7   answered_correctly              int8   \n 8   prior_question_elapsed_time     float32\n 9   prior_question_had_explanation  object \ndtypes: bool(1), float32(1), int16(2), int32(1), int64(2), int8(2), object(1)\nmemory usage: 3.7+ GB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_pickle(TRAIN_PATH)\n",
    "train_df.info()"
   ]
  },
  {
   "source": [
    "## 2. Data Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Remove lectures\n",
    "train_df = train_df[train_df.content_type_id == 0]\n",
    "\n",
    "# Find unique skills\n",
    "skills = train_df[\"content_id\"].unique()\n",
    "n_skill = len(skills)\n",
    "\n",
    "\"\"\"\n",
    "1. Fill NA elapsed time\n",
    "2. Change unit to seconds\n",
    "3. Crop out the top 300 elapsed times\n",
    "\"\"\"\n",
    "train_df.prior_question_elapsed_time.fillna(0, inplace=True)\n",
    "train_df.prior_question_elapsed_time /= 1000\n",
    "train_df.prior_question_elapsed_time = train_df.prior_question_elapsed_time.astype(np.int)\n",
    "# df_train.prior_question_elapsed_time.clip(upper=300)\n",
    "\n",
    "# Group by user\n",
    "user_df = train_df[[\"user_id\", \"content_id\", \"answered_correctly\", \"prior_question_elapsed_time\", \"task_container_id\"]].groupby('user_id').apply(lambda r: (\n",
    "            r.content_id.values,\n",
    "            r.answered_correctly.values,\n",
    "            r.prior_question_elapsed_time.values, \n",
    "            r.task_container_id.values\n",
    "            ))\n",
    "\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "## 3. Creating the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiiidDataset(Dataset):\n",
    "    def __init__(self, user_df, max_seq=100):\n",
    "        super(RiiidDataset, self).__init__()\n",
    "        self.user_df = user_df\n",
    "        self.max_seq = max_seq\n",
    "        self.user_ids = []\n",
    "\n",
    "        for user_id in self.user_df.index:\n",
    "            exercise_id, answered_correctly, elapsed_time, container_id = self.user_df[user_id]\n",
    "            # Remove users who did less than 10 exercises\n",
    "            ex_num = len(exercise_id) # number of exercises\n",
    "            if ex_num >= 10:\n",
    "                self.user_ids.append(user_id)\n",
    "                # idx = min(ex_num, max_seq)\n",
    "                # entry = (exercise_id[:idx], answered_correctly[:idx],elapsed_time[:idx], container_id[:idx])\n",
    "                # self.dataset.append(entry)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx]\n",
    "        exercise_id, answered_correctly, elapsed_time, container_id = self.user_df[user_id]\n",
    "\n",
    "        ex_num = len(exercise_id)\n",
    "\n",
    "        exercise_id_temp = np.zeros(self.max_seq, dtype=int)\n",
    "        answered_correctly_temp = np.zeros(self.max_seq, dtype=int)\n",
    "        elapsed_time_temp = np.zeros(self.max_seq, dtype=int)\n",
    "        container_id_temp = np.zeros(self.max_seq, dtype=int)\n",
    "\n",
    "        if ex_num >= self.max_seq:\n",
    "            exercise_id_temp[:] = exercise_id[-self.max_seq:]\n",
    "            answered_correctly_temp[:] = answered_correctly[-self.max_seq:]\n",
    "            elapsed_time_temp[:] = elapsed_time[-self.max_seq:]\n",
    "            container_id_temp[:] = container_id[-self.max_seq:]\n",
    "        else:\n",
    "            exercise_id_temp[-ex_num:] = exercise_id\n",
    "            answered_correctly_temp[-ex_num:] = answered_correctly\n",
    "            elapsed_time_temp[-ex_num:] = elapsed_time\n",
    "            container_id_temp[-ex_num:] = container_id\n",
    "\n",
    "        exercise_ids = exercise_id_temp\n",
    "        labels = answered_correctly_temp\n",
    "        elapsed_times = elapsed_time_temp\n",
    "        container_ids = container_id_temp\n",
    "\n",
    "        return exercise_id_temp, answered_correctly_temp, answered_correctly_temp, container_id_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train, val = train_test_split(user_df, test_size=0.2)\n",
    "\n",
    "train_dataset = RiiidDataset(train, max_seq=MAX_SEQ)\n",
    "val_dataset = RiiidDataset(val, max_seq=MAX_SEQ)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=8,\n",
    "                            shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=8,\n",
    "                        shuffle=True)\n",
    "del train, val, train_dataset, val_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "## 4. Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Layer\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(FFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, in_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features, in_features)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        SAINT+ Model has 3 input embeddings for the encoder part\":\n",
    "        1. Exercise ID\n",
    "        2. Position\n",
    "        3. Part (10000 unique task container id)\n",
    "\"\"\"\n",
    "class EncoderEmbedding(nn.Module):\n",
    "    def __init__(self, n_exercises, n_categories, n_dims, seq_len):\n",
    "        super(EncoderEmbedding, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.seq_len = seq_len\n",
    "        self.exercise_embedding = nn.Embedding(n_exercises, n_dims)\n",
    "        self.position_embediing = nn.Embedding(seq_len, n_dims)\n",
    "        self.part_embedding = nn.Embedding(n_categories, n_dims)\n",
    "\n",
    "    def forward(self, exercises, categories):\n",
    "        e = self.exercise_embedding(exercises)\n",
    "        c = self.part_embedding(categories)\n",
    "        seq = torch.arange(self.seq_len, device=device).unsqueeze(0)\n",
    "        p = self.position_embediing(seq)\n",
    "        return p + c + e\n",
    "\n",
    "\"\"\"\n",
    "        SAINT+ Model has 4 input embeddings for the decoder part\":\n",
    "        1. Correctness (response)\n",
    "        2. Position\n",
    "        3. Elapsed Time\n",
    "        4. Lag Time (to be implemented)\n",
    "\"\"\"\n",
    "class DecoderEmbedding(nn.Module):\n",
    "    def __init__(self, n_responses, n_dims, seq_len):\n",
    "        super(DecoderEmbedding, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.seq_len = seq_len\n",
    "        self.response_embedding = nn.Embedding(n_responses, n_dims)\n",
    "        self.elapsed_time_embedding = nn.Embedding(300, n_dims)\n",
    "        self.position_embedding = nn.Embedding(seq_len, n_dims)\n",
    "\n",
    "    def forward(self, responses, elapsed_times):\n",
    "        e = self.response_embedding(responses)\n",
    "        t = self.elapsed_time_embedding(elapsed_times)\n",
    "        seq = torch.arange(self.seq_len, device=device).unsqueeze(0)\n",
    "        p = self.position_embedding(seq)\n",
    "        return p + e + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Copied*\n",
    "class StackedNMultiHeadAttention(nn.Module):\n",
    "  def __init__(self,n_stacks,n_dims,n_heads,seq_len,n_multihead=1,dropout=0.2):\n",
    "    super(StackedNMultiHeadAttention,self).__init__()\n",
    "    self.n_stacks = n_stacks\n",
    "    self.n_multihead = n_multihead\n",
    "    self.n_dims = n_dims \n",
    "    self.norm_layers = nn.LayerNorm(n_dims)\n",
    "    #n_stacks has n_multiheads each\n",
    "    self.multihead_layers = nn.ModuleList(n_stacks*[nn.ModuleList(n_multihead*[nn.MultiheadAttention(embed_dim = n_dims,\n",
    "                                                      num_heads = n_heads,\n",
    "                                                        dropout = dropout),]),])\n",
    "    self.ffn = nn.ModuleList(n_stacks*[FFN(n_dims)])\n",
    "    self.mask = torch.triu(torch.ones(seq_len,seq_len),diagonal=1).to(dtype=torch.bool)\n",
    "  \n",
    "  def forward(self,input_q,input_k,input_v,encoder_output=None,break_layer=None):\n",
    "    for stack in range(self.n_stacks):\n",
    "        for multihead in range(self.n_multihead):\n",
    "          norm_q = self.norm_layers(input_q)\n",
    "          norm_k = self.norm_layers(input_k)\n",
    "          norm_v = self.norm_layers(input_v) \n",
    "          heads_output,_ = self.multihead_layers[stack][multihead](query=norm_q.permute(1,0,2),\n",
    "                                                                    key=norm_k.permute(1,0,2),\n",
    "                                                                    value=norm_v.permute(1,0,2),\n",
    "                                                                    attn_mask=self.mask.to(device))\n",
    "          heads_output = heads_output.permute(1,0,2)\n",
    "          #assert encoder_output != None and break_layer is not None     \n",
    "          if encoder_output != None and multihead == break_layer:\n",
    "            assert break_layer <= multihead, \" break layer should be less than multihead layers and postive integer\"\n",
    "            input_k = input_v = encoder_output\n",
    "            input_q =input_q + heads_output\n",
    "          else:\n",
    "            input_q =input_q+ heads_output\n",
    "            input_k =input_k+ heads_output\n",
    "            input_v =input_v +heads_output\n",
    "        last_norm = self.norm_layers(heads_output)\n",
    "        ffn_output = self.ffn[stack](last_norm)\n",
    "        ffn_output =ffn_output+ heads_output\n",
    "    return ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAINT(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_DIMS):\n",
    "        super(SAINT, self).__init__()\n",
    "        self.n_skill = n_skill\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.encoder_embedding = EncoderEmbedding(n_skill, 10000, embed_dim, max_seq)\n",
    "        self.decoder_embedding = DecoderEmbedding(n_skill, embed_dim, max_seq)\n",
    "\n",
    "        self.encoder_layer = StackedNMultiHeadAttention(n_stacks=NUM_DECODER,n_dims=EMBED_DIMS,n_heads=DEC_HEADS,seq_len=MAX_SEQ,n_multihead=1,dropout=0.2)\n",
    "        self.decoder_layer = StackedNMultiHeadAttention(n_stacks=NUM_ENCODER,n_dims=EMBED_DIMS, n_heads=ENC_HEADS,seq_len=MAX_SEQ,n_multihead=2,dropout=0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def future_mask(self, seq_length):\n",
    "        future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "        return torch.from_numpy(future_mask)\n",
    "    \n",
    "    def forward(self, exercise_id, label, elapsed_time, container_id):\n",
    "        enc_emb = self.encoder_embedding(exercise_id, container_id)\n",
    "        dec_emb = self.decoder_embedding(label, elapsed_time)\n",
    "\n",
    "        # att_mask = self.future_mask(len(exercise_id)).to(device)\n",
    "        # att_mask =att_mask\n",
    "        enc_output = self.encoder_layer(enc_emb, enc_emb, enc_emb)\n",
    "\n",
    "        dec_output = self.decoder_layer(dec_emb, dec_emb, dec_emb, encoder_output=enc_emb, break_layer=1)\n",
    "        # dec_output = self.ffn(dec_output)\n",
    "\n",
    "        output = self.pred(dec_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3129.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "669be14f56e4454bb40967c3e33874c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [17651563, 31280300]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d93c5ddfc283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_corrects\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         return _average_binary_score(partial(_binary_roc_auc_score,\n\u001b[0m\u001b[1;32m    391\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    392\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    224\u001b[0m                          \"is not defined in that case.\")\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0m\u001b[1;32m    227\u001b[0m                             sample_weight=sample_weight)\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \"\"\"\n\u001b[0;32m--> 775\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    776\u001b[0m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [17651563, 31280300]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = SAINT(n_skill)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=0.005)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "train_iterator = train_loader\n",
    "optim = optimizer\n",
    "model.train()\n",
    "\n",
    "train_loss = []\n",
    "num_corrects = 0\n",
    "num_total = 0\n",
    "labels = []\n",
    "outs = []\n",
    "\n",
    "tbar = tqdm(train_iterator)\n",
    "for item in tbar:\n",
    "    exercise_ids = item[0].to(device).long()\n",
    "    label = item[1].to(device).long()\n",
    "    elapsed_times = item[2].to(device).long()\n",
    "    container_ids= item[3].to(device).long()\n",
    "\n",
    "    target_mask = (exercise_ids != 0)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    output = model(exercise_ids, label, elapsed_times, container_ids)\n",
    "    \n",
    "    outputs = torch.masked_select(output[:,-1], target_mask)\n",
    "    label = torch.masked_select(label, target_mask)\n",
    "    \n",
    "    loss = criterion(outputs.float(), label.float())\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    train_loss.append(loss.item())\n",
    "    pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "    \n",
    "    num_corrects += (pred == label).sum().item()\n",
    "    num_total += len(label)\n",
    "\n",
    "    labels.extend(label.view(-1).data.cpu().numpy())\n",
    "    outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "    tbar.set_description('loss - {:.4f}'.format(loss))\n",
    "\n",
    "acc = num_corrects / num_total\n",
    "auc = roc_auc_score(labels, outs)\n",
    "loss = np.average(train_loss)\n",
    "print(\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\".format(epoch, loss, acc, auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, val_iterator, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    tbar = tqdm(val_iterator)\n",
    "    for item in tbar:\n",
    "        x = item[0].to(device).long()\n",
    "        target_id = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "        target_mask = (target_id != 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, atten_weight = model(x, target_id)\n",
    "        \n",
    "        output = torch.masked_select(output, target_mask)\n",
    "        label = torch.masked_select(label, target_mask)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "        tbar.set_description('loss - {:.4f}'.format(loss))\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.average(train_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "source": [
    "## 5. Inferencing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1222.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cedc099b4c7b476f898e585a037fdf81"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "epoch - 0 train_loss - 0.59 acc - 0.684 auc - 0.731\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=306.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "826561cbde04493697433c122c3881ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "epoch - 0 val_loss - 0.58 acc - 0.697 auc - 0.753\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1222.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c47be2da2ae94cf590192a97033d862d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "epoch - 1 train_loss - 0.57 acc - 0.699 auc - 0.755\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=306.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31727bd20e2e49499cec67b1b29ff21d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "epoch - 1 val_loss - 0.57 acc - 0.699 auc - 0.756\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1222.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35fa175e90b64db381c0ea71970c0d53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-049dde4ac68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlast_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-743bb4889dd6>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iterator, optim, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "over_fit = 0\n",
    "last_auc = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_auc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\".format(epoch, train_loss, train_acc, train_auc))\n",
    "    \n",
    "    val_loss, avl_acc, val_auc = val_epoch(model, val_loader, criterion, device)\n",
    "    print(\"epoch - {} val_loss - {:.2f} acc - {:.3f} auc - {:.3f}\".format(epoch, val_loss, avl_acc, val_auc))\n",
    "    \n",
    "    if val_auc > last_auc:\n",
    "        last_auc = val_auc\n",
    "        over_fit = 0\n",
    "    else:\n",
    "        over_fit += 1\n",
    "        \n",
    "    \n",
    "    if over_fit >= 2:\n",
    "        print(\"early stop epoch \", epoch)\n",
    "        break"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}